+-----------------+-------------------------+
|    Parameter    |          Value          |
+=================+=========================+
| Batch size      | 128                     |
+-----------------+-------------------------+
| Dataset         | NCI109                  |
+-----------------+-------------------------+
| Dropout ratio   | 0.5                     |
+-----------------+-------------------------+
| Epochs          | 10000                   |
+-----------------+-------------------------+
| Exp name        | NCI109_Hie              |
+-----------------+-------------------------+
| Gpu index       | 0                       |
+-----------------+-------------------------+
| Hid             | 128                     |
+-----------------+-------------------------+
| Lr              | 0.0005                  |
+-----------------+-------------------------+
| Model           | SAGPooling_Hierarchical |
+-----------------+-------------------------+
| Patience        | 40                      |
+-----------------+-------------------------+
| Pooling ratio   | 0.5                     |
+-----------------+-------------------------+
| Seed            | 16                      |
+-----------------+-------------------------+
| Test batch size | 1                       |
+-----------------+-------------------------+
| Weight decay    | 0.0001                  |
+-----------------+-------------------------+
Using GPU: 0
SAGPooling_Hierarchical(
  (conv1): GCNConv(38, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): SAGPooling(GCNConv, 128, ratio=0.5, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 79428
Using Adam

Epoch #000, Train_Loss: [0.6928, 0.6921, 0.6950, 0.6937, 0.6945, 0.6925, 0.6926, 0.6923, 0.6928, 0.6938, 0.6941, 0.6928, 0.6942, 0.6922, 0.6957, 0.6928, 0.6934, 0.6932, 0.6920, 0.6930, 0.6923, 0.6928, 0.6938, 0.6932, 0.6945, 0.6927]
Val_Loss: 0.693466, Val_Acc: 0.487864
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #001, Train_Loss: [0.6938, 0.6919, 0.6933, 0.6932, 0.6922, 0.6938, 0.6945, 0.6930, 0.6925, 0.6931, 0.6929, 0.6942, 0.6923, 0.6932, 0.6938, 0.6929, 0.6934, 0.6924, 0.6931, 0.6924, 0.6927, 0.6932, 0.6928, 0.6932, 0.6924, 0.6939]
Val_Loss: 0.693300, Val_Acc: 0.487864
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #002, Train_Loss: [0.6928, 0.6934, 0.6937, 0.6910, 0.6923, 0.6936, 0.6907, 0.6936, 0.6947, 0.6917, 0.6927, 0.6935, 0.6901, 0.6937, 0.6938, 0.6920, 0.6908, 0.6941, 0.6983, 0.6937, 0.6932, 0.6940, 0.6935, 0.6912, 0.6923, 0.6924]
Val_Loss: 0.693062, Val_Acc: 0.500000
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #003, Train_Loss: [0.6922, 0.6920, 0.6933, 0.6926, 0.6918, 0.6926, 0.6918, 0.6930, 0.6925, 0.6913, 0.6929, 0.6940, 0.6929, 0.6894, 0.6925, 0.6944, 0.6932, 0.6923, 0.6920, 0.6926, 0.6933, 0.6896, 0.6919, 0.6913, 0.6923, 0.6954]
Val_Loss: 0.692596, Val_Acc: 0.538835
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #004, Train_Loss: [0.6931, 0.6920, 0.6935, 0.6948, 0.6930, 0.6925, 0.6924, 0.6937, 0.6910, 0.6922, 0.6898, 0.6913, 0.6912, 0.6937, 0.6886, 0.6903, 0.6904, 0.6915, 0.6950, 0.6880, 0.6904, 0.6933, 0.6911, 0.6838, 0.6959, 0.6964]
Val_Loss: 0.691881, Val_Acc: 0.526699
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #005, Train_Loss: [0.6924, 0.6901, 0.6876, 0.6895, 0.6891, 0.6909, 0.6874, 0.6910, 0.6878, 0.6881, 0.6833, 0.6838, 0.6806, 0.6837, 0.6932, 0.6892, 0.6908, 0.6935, 0.6959, 0.6859, 0.6980, 0.6786, 0.6865, 0.6942, 0.6844, 0.6862]
Val_Loss: 0.688208, Val_Acc: 0.558252
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #006, Train_Loss: [0.6969, 0.6822, 0.6867, 0.6714, 0.6885, 0.6782, 0.6787, 0.6967, 0.6837, 0.6976, 0.6767, 0.6907, 0.6887, 0.6671, 0.6834, 0.6817, 0.6935, 0.6903, 0.6689, 0.6769, 0.6947, 0.6833, 0.6681, 0.6743, 0.6732, 0.7009]
Val_Loss: 0.688703, Val_Acc: 0.541262

Epoch #007, Train_Loss: [0.6902, 0.6830, 0.6862, 0.6850, 0.6896, 0.6864, 0.6911, 0.6873, 0.6757, 0.6632, 0.6956, 0.6870, 0.6776, 0.6812, 0.6854, 0.6738, 0.6688, 0.6637, 0.6914, 0.7017, 0.6802, 0.6976, 0.6730, 0.6930, 0.6688, 0.6702]
Val_Loss: 0.685733, Val_Acc: 0.575243
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #008, Train_Loss: [0.6749, 0.6966, 0.7180, 0.6913, 0.6652, 0.6774, 0.6808, 0.6448, 0.6846, 0.6791, 0.6844, 0.6754, 0.6644, 0.6931, 0.6919, 0.6869, 0.6845, 0.6876, 0.6776, 0.6833, 0.6709, 0.6852, 0.6786, 0.6889, 0.6782, 0.6706]
Val_Loss: 0.683799, Val_Acc: 0.575243
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #009, Train_Loss: [0.6684, 0.6816, 0.6641, 0.6624, 0.6806, 0.7008, 0.6994, 0.7073, 0.6781, 0.6621, 0.6871, 0.6821, 0.6769, 0.6758, 0.6908, 0.6634, 0.6715, 0.6925, 0.6766, 0.6563, 0.6862, 0.6845, 0.6468, 0.6897, 0.6864, 0.6823]
Val_Loss: 0.681067, Val_Acc: 0.587379
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #010, Train_Loss: [0.6812, 0.6624, 0.6777, 0.6661, 0.6828, 0.6808, 0.6951, 0.6631, 0.6645, 0.7022, 0.6963, 0.6547, 0.6789, 0.6685, 0.6663, 0.6501, 0.6826, 0.6774, 0.6806, 0.6778, 0.6644, 0.6640, 0.6869, 0.6583, 0.6759, 0.6618]
Val_Loss: 0.672943, Val_Acc: 0.609223
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #011, Train_Loss: [0.6807, 0.6944, 0.6663, 0.6357, 0.7024, 0.6661, 0.6745, 0.6787, 0.6893, 0.6544, 0.6726, 0.6558, 0.6871, 0.6388, 0.6343, 0.6288, 0.6613, 0.7005, 0.6581, 0.6482, 0.6410, 0.6889, 0.6591, 0.6405, 0.6739, 0.6392]
Val_Loss: 0.654264, Val_Acc: 0.618932
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #012, Train_Loss: [0.6398, 0.6724, 0.6641, 0.6307, 0.6415, 0.6525, 0.6521, 0.6133, 0.6085, 0.6508, 0.6754, 0.6333, 0.6463, 0.6129, 0.6526, 0.7408, 0.6760, 0.6417, 0.6612, 0.6685, 0.6880, 0.6627, 0.6444, 0.6846, 0.6579, 0.6139]
Val_Loss: 0.645112, Val_Acc: 0.648058
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #013, Train_Loss: [0.6279, 0.6595, 0.6418, 0.6656, 0.6703, 0.6408, 0.6386, 0.6328, 0.6750, 0.6385, 0.6298, 0.6323, 0.6413, 0.6549, 0.6665, 0.6557, 0.6184, 0.6310, 0.6436, 0.6343, 0.6045, 0.6557, 0.6534, 0.6097, 0.6341, 0.6264]
Val_Loss: 0.639476, Val_Acc: 0.621359
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #014, Train_Loss: [0.6156, 0.6622, 0.6329, 0.5702, 0.6240, 0.6675, 0.6246, 0.6791, 0.6468, 0.6657, 0.6325, 0.6225, 0.6153, 0.6043, 0.6459, 0.6304, 0.7084, 0.6082, 0.6448, 0.6580, 0.6122, 0.6526, 0.6518, 0.7031, 0.5721, 0.6597]
Val_Loss: 0.626335, Val_Acc: 0.623786
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #015, Train_Loss: [0.6352, 0.6611, 0.6028, 0.6203, 0.6265, 0.6122, 0.6673, 0.6100, 0.6479, 0.6546, 0.6133, 0.6406, 0.6292, 0.6245, 0.6646, 0.6035, 0.6671, 0.6358, 0.5951, 0.6193, 0.6167, 0.6681, 0.6537, 0.6730, 0.5542, 0.5809]
Val_Loss: 0.631349, Val_Acc: 0.621359

Epoch #016, Train_Loss: [0.6537, 0.6032, 0.6285, 0.6290, 0.6446, 0.6302, 0.6424, 0.6731, 0.5865, 0.6421, 0.6200, 0.6177, 0.7083, 0.6876, 0.6305, 0.5917, 0.6301, 0.6009, 0.6202, 0.6468, 0.6327, 0.6918, 0.6590, 0.5975, 0.6720, 0.6207]
Val_Loss: 0.630686, Val_Acc: 0.626214

Epoch #017, Train_Loss: [0.6332, 0.6310, 0.6358, 0.5761, 0.6944, 0.6414, 0.5904, 0.6166, 0.6147, 0.6174, 0.6147, 0.6432, 0.5970, 0.6507, 0.5776, 0.6501, 0.5729, 0.6918, 0.6330, 0.6545, 0.6256, 0.6302, 0.6207, 0.5770, 0.5582, 0.6562]
Val_Loss: 0.633066, Val_Acc: 0.628641

Epoch #018, Train_Loss: [0.5574, 0.5767, 0.6450, 0.6514, 0.7030, 0.5958, 0.6567, 0.6610, 0.6310, 0.6037, 0.6857, 0.6248, 0.6147, 0.6111, 0.6617, 0.6707, 0.6187, 0.6056, 0.5962, 0.6140, 0.6254, 0.6118, 0.6201, 0.6071, 0.6471, 0.5671]
Val_Loss: 0.616877, Val_Acc: 0.682039
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #019, Train_Loss: [0.6126, 0.5831, 0.5902, 0.5862, 0.6356, 0.6240, 0.6094, 0.6107, 0.6478, 0.6211, 0.6796, 0.6277, 0.5726, 0.6370, 0.6420, 0.6350, 0.5846, 0.5994, 0.6083, 0.6413, 0.6038, 0.5854, 0.5683, 0.6296, 0.6390, 0.6914]
Val_Loss: 0.621753, Val_Acc: 0.682039

Epoch #020, Train_Loss: [0.6430, 0.6079, 0.5772, 0.5966, 0.6417, 0.6326, 0.6352, 0.6091, 0.6417, 0.6369, 0.5882, 0.6482, 0.5918, 0.6406, 0.6710, 0.6018, 0.5747, 0.5485, 0.6287, 0.6535, 0.5812, 0.6634, 0.6587, 0.6285, 0.5967, 0.6277]
Val_Loss: 0.647938, Val_Acc: 0.577670

Epoch #021, Train_Loss: [0.6352, 0.6338, 0.5927, 0.6649, 0.6135, 0.6369, 0.5783, 0.6750, 0.5953, 0.6209, 0.6167, 0.6934, 0.5922, 0.5795, 0.6028, 0.6395, 0.5820, 0.6051, 0.6402, 0.5922, 0.6484, 0.6185, 0.5899, 0.5967, 0.5615, 0.6953]
Val_Loss: 0.615591, Val_Acc: 0.677184
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #022, Train_Loss: [0.6297, 0.5764, 0.6068, 0.6093, 0.5921, 0.6539, 0.6086, 0.5942, 0.5804, 0.5753, 0.6827, 0.6618, 0.6218, 0.6188, 0.6412, 0.6970, 0.7130, 0.5890, 0.5921, 0.6136, 0.6173, 0.6297, 0.5875, 0.5909, 0.6080, 0.5357]
Val_Loss: 0.625011, Val_Acc: 0.660194

Epoch #023, Train_Loss: [0.5857, 0.6055, 0.5437, 0.5821, 0.5741, 0.6562, 0.6331, 0.6168, 0.6345, 0.6013, 0.6775, 0.6236, 0.5735, 0.6011, 0.6366, 0.6826, 0.6277, 0.5861, 0.6059, 0.5833, 0.5983, 0.6673, 0.5453, 0.6024, 0.6232, 0.5566]
Val_Loss: 0.614187, Val_Acc: 0.674757
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #024, Train_Loss: [0.5868, 0.6350, 0.6036, 0.6337, 0.6293, 0.5880, 0.6095, 0.6028, 0.5820, 0.5559, 0.6298, 0.6109, 0.6387, 0.5896, 0.6818, 0.6561, 0.5888, 0.6004, 0.5811, 0.6188, 0.5846, 0.6239, 0.6429, 0.6326, 0.6237, 0.5724]
Val_Loss: 0.626502, Val_Acc: 0.669903

Epoch #025, Train_Loss: [0.5764, 0.5669, 0.6024, 0.5760, 0.6248, 0.6034, 0.5795, 0.6504, 0.6368, 0.5841, 0.6083, 0.6556, 0.6513, 0.6133, 0.6765, 0.5441, 0.5738, 0.6307, 0.5746, 0.6385, 0.6171, 0.6312, 0.6080, 0.5942, 0.6160, 0.5295]
Val_Loss: 0.621062, Val_Acc: 0.672330

Epoch #026, Train_Loss: [0.5844, 0.6203, 0.6088, 0.5756, 0.6088, 0.6384, 0.6138, 0.6571, 0.6474, 0.5909, 0.5747, 0.5963, 0.5845, 0.5894, 0.6262, 0.5859, 0.6215, 0.5998, 0.5971, 0.6321, 0.6290, 0.6143, 0.5973, 0.6051, 0.6163, 0.5817]
Val_Loss: 0.627951, Val_Acc: 0.638350

Epoch #027, Train_Loss: [0.5424, 0.6151, 0.5866, 0.6054, 0.5697, 0.6150, 0.6525, 0.5946, 0.6182, 0.6772, 0.6220, 0.6205, 0.6119, 0.6307, 0.5900, 0.5637, 0.5978, 0.5823, 0.6270, 0.5340, 0.6150, 0.6112, 0.6655, 0.5798, 0.6195, 0.6459]
Val_Loss: 0.609629, Val_Acc: 0.699029
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #028, Train_Loss: [0.6271, 0.5773, 0.5745, 0.6066, 0.5845, 0.6428, 0.6561, 0.6052, 0.6345, 0.6182, 0.6311, 0.6028, 0.6622, 0.5849, 0.5700, 0.5685, 0.6155, 0.6017, 0.5969, 0.6597, 0.5678, 0.6255, 0.6150, 0.5876, 0.6046, 0.5887]
Val_Loss: 0.610175, Val_Acc: 0.706311

Epoch #029, Train_Loss: [0.6256, 0.6163, 0.6096, 0.5253, 0.6286, 0.6383, 0.5837, 0.6382, 0.6293, 0.6172, 0.5550, 0.5828, 0.5981, 0.5256, 0.5823, 0.6221, 0.6301, 0.5743, 0.5925, 0.5895, 0.6213, 0.6371, 0.6111, 0.6355, 0.5995, 0.6481]
Val_Loss: 0.611470, Val_Acc: 0.701456

Epoch #030, Train_Loss: [0.5858, 0.6080, 0.6564, 0.6037, 0.6157, 0.6138, 0.6198, 0.6395, 0.5811, 0.5688, 0.6099, 0.5941, 0.6540, 0.5698, 0.5777, 0.6041, 0.5609, 0.6258, 0.5861, 0.5639, 0.6257, 0.6512, 0.5447, 0.6264, 0.6134, 0.5520]
Val_Loss: 0.604490, Val_Acc: 0.706311
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #031, Train_Loss: [0.5277, 0.5754, 0.6695, 0.5827, 0.6020, 0.6196, 0.5886, 0.6539, 0.6027, 0.5785, 0.5929, 0.5887, 0.6137, 0.6024, 0.5982, 0.6116, 0.6348, 0.5929, 0.6098, 0.5883, 0.6239, 0.6078, 0.6177, 0.5659, 0.5607, 0.5577]
Val_Loss: 0.601470, Val_Acc: 0.708738
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #032, Train_Loss: [0.6415, 0.5924, 0.5996, 0.6429, 0.6310, 0.6081, 0.6166, 0.5802, 0.5845, 0.6230, 0.5579, 0.6228, 0.5691, 0.5950, 0.6186, 0.6047, 0.5312, 0.5816, 0.5932, 0.5820, 0.6048, 0.5832, 0.6448, 0.6530, 0.6279, 0.6254]
Val_Loss: 0.628544, Val_Acc: 0.650485

Epoch #033, Train_Loss: [0.5965, 0.6742, 0.5670, 0.5599, 0.6542, 0.6697, 0.6069, 0.5835, 0.5872, 0.5443, 0.6467, 0.5274, 0.5853, 0.5980, 0.5944, 0.5953, 0.6141, 0.5741, 0.6657, 0.6157, 0.5744, 0.5764, 0.6212, 0.6419, 0.5706, 0.6017]
Val_Loss: 0.619742, Val_Acc: 0.665049

Epoch #034, Train_Loss: [0.6009, 0.6174, 0.5895, 0.6800, 0.6992, 0.5753, 0.6054, 0.6314, 0.5776, 0.5520, 0.6445, 0.6712, 0.6717, 0.6176, 0.5889, 0.6088, 0.6021, 0.5887, 0.5975, 0.6196, 0.6526, 0.6163, 0.6471, 0.6083, 0.6328, 0.6170]
Val_Loss: 0.612695, Val_Acc: 0.686893

Epoch #035, Train_Loss: [0.6371, 0.5912, 0.6525, 0.5867, 0.5766, 0.6127, 0.6396, 0.6227, 0.6179, 0.5953, 0.5948, 0.6005, 0.6397, 0.5785, 0.6580, 0.5783, 0.5639, 0.6045, 0.6435, 0.5921, 0.5567, 0.5841, 0.6615, 0.5464, 0.6212, 0.5955]
Val_Loss: 0.623373, Val_Acc: 0.657767

Epoch #036, Train_Loss: [0.5988, 0.6335, 0.6258, 0.6221, 0.6430, 0.5762, 0.5932, 0.5916, 0.6369, 0.6633, 0.6541, 0.6436, 0.6131, 0.5389, 0.5787, 0.5523, 0.6272, 0.6412, 0.6057, 0.5868, 0.5941, 0.5440, 0.5813, 0.5430, 0.6689, 0.6037]
Val_Loss: 0.603577, Val_Acc: 0.684466

Epoch #037, Train_Loss: [0.5702, 0.5630, 0.5728, 0.6115, 0.6055, 0.6177, 0.5599, 0.6150, 0.6111, 0.5972, 0.5707, 0.5737, 0.5755, 0.5801, 0.6043, 0.6165, 0.5698, 0.6066, 0.6433, 0.6309, 0.5681, 0.5937, 0.6126, 0.6250, 0.5798, 0.6294]
Val_Loss: 0.610141, Val_Acc: 0.694175

Epoch #038, Train_Loss: [0.5883, 0.5559, 0.5523, 0.6328, 0.5865, 0.5755, 0.5614, 0.5738, 0.6304, 0.6250, 0.5513, 0.5915, 0.6179, 0.5824, 0.6229, 0.6469, 0.6516, 0.5881, 0.5164, 0.5932, 0.6087, 0.6287, 0.5520, 0.5636, 0.5951, 0.6061]
Val_Loss: 0.605832, Val_Acc: 0.684466

Epoch #039, Train_Loss: [0.6118, 0.5383, 0.6390, 0.5546, 0.5934, 0.6222, 0.6404, 0.5957, 0.5792, 0.5934, 0.5523, 0.5926, 0.6081, 0.6161, 0.6297, 0.5083, 0.6257, 0.6170, 0.5284, 0.6541, 0.5935, 0.5324, 0.6273, 0.6411, 0.5784, 0.5428]
Val_Loss: 0.602631, Val_Acc: 0.701456

Epoch #040, Train_Loss: [0.5873, 0.6349, 0.6026, 0.6088, 0.5722, 0.5821, 0.6060, 0.5497, 0.6043, 0.5719, 0.5954, 0.5967, 0.5781, 0.6357, 0.6120, 0.5640, 0.6091, 0.5884, 0.6189, 0.5879, 0.5399, 0.5344, 0.5888, 0.5716, 0.6178, 0.6159]
Val_Loss: 0.606887, Val_Acc: 0.686893

Epoch #041, Train_Loss: [0.6002, 0.5738, 0.5805, 0.6599, 0.5547, 0.5635, 0.6568, 0.5638, 0.5829, 0.5906, 0.5490, 0.5599, 0.6020, 0.5638, 0.6175, 0.5849, 0.5814, 0.5893, 0.5741, 0.5886, 0.6233, 0.6062, 0.5785, 0.6270, 0.6010, 0.6020]
Val_Loss: 0.609904, Val_Acc: 0.689320

Epoch #042, Train_Loss: [0.5941, 0.6338, 0.5467, 0.6009, 0.5343, 0.5582, 0.5910, 0.6098, 0.5307, 0.5799, 0.6448, 0.6064, 0.5672, 0.6274, 0.5997, 0.5817, 0.6334, 0.6194, 0.5561, 0.6001, 0.5774, 0.5697, 0.6405, 0.5674, 0.6042, 0.5395]
Val_Loss: 0.607693, Val_Acc: 0.686893

Epoch #043, Train_Loss: [0.5390, 0.5283, 0.6086, 0.5643, 0.6512, 0.5371, 0.5754, 0.6311, 0.6352, 0.5792, 0.5828, 0.5597, 0.5486, 0.6055, 0.5956, 0.6085, 0.5699, 0.6158, 0.6414, 0.5881, 0.6188, 0.5551, 0.6329, 0.6547, 0.5829, 0.5961]
Val_Loss: 0.605936, Val_Acc: 0.694175

Epoch #044, Train_Loss: [0.5747, 0.6053, 0.5351, 0.5886, 0.6113, 0.5676, 0.5598, 0.6108, 0.5981, 0.5328, 0.5621, 0.6563, 0.5669, 0.6531, 0.6697, 0.5506, 0.6103, 0.6147, 0.5497, 0.6164, 0.5780, 0.6222, 0.5967, 0.5930, 0.5723, 0.5600]
Val_Loss: 0.603376, Val_Acc: 0.684466

Epoch #045, Train_Loss: [0.5538, 0.5530, 0.6291, 0.5861, 0.5526, 0.5590, 0.6254, 0.6132, 0.6141, 0.5898, 0.5575, 0.5280, 0.5717, 0.5875, 0.6167, 0.5678, 0.5373, 0.5747, 0.5918, 0.7128, 0.6105, 0.5458, 0.6100, 0.6209, 0.5789, 0.5698]
Val_Loss: 0.608728, Val_Acc: 0.696602

Epoch #046, Train_Loss: [0.6539, 0.6185, 0.5330, 0.5836, 0.5706, 0.6072, 0.6068, 0.5792, 0.5323, 0.6147, 0.5384, 0.5804, 0.5530, 0.5750, 0.5828, 0.6238, 0.5618, 0.6120, 0.6748, 0.5330, 0.6128, 0.5106, 0.6202, 0.6022, 0.5801, 0.5818]
Val_Loss: 0.614494, Val_Acc: 0.677184

Epoch #047, Train_Loss: [0.6150, 0.5795, 0.5422, 0.5949, 0.5923, 0.6273, 0.6583, 0.5514, 0.6212, 0.5613, 0.6007, 0.5931, 0.5088, 0.5984, 0.5836, 0.6427, 0.5690, 0.5520, 0.5105, 0.6111, 0.5843, 0.6285, 0.5924, 0.5803, 0.6506, 0.5717]
Val_Loss: 0.609795, Val_Acc: 0.684466

Epoch #048, Train_Loss: [0.5517, 0.5683, 0.5807, 0.6037, 0.5712, 0.6106, 0.6614, 0.5816, 0.6092, 0.5575, 0.6526, 0.5706, 0.5846, 0.5987, 0.4978, 0.6015, 0.5684, 0.5832, 0.5743, 0.6202, 0.6175, 0.5532, 0.6040, 0.5537, 0.6008, 0.6037]
Val_Loss: 0.608599, Val_Acc: 0.674757

Epoch #049, Train_Loss: [0.6352, 0.5940, 0.5786, 0.6155, 0.5890, 0.5912, 0.6081, 0.5808, 0.5562, 0.5702, 0.5260, 0.5340, 0.5747, 0.6113, 0.6146, 0.5477, 0.6182, 0.5655, 0.5305, 0.6352, 0.6135, 0.5274, 0.5820, 0.6715, 0.5579, 0.6933]
Val_Loss: 0.628914, Val_Acc: 0.645631

Epoch #050, Train_Loss: [0.5755, 0.6179, 0.5484, 0.5752, 0.6278, 0.6118, 0.5911, 0.5290, 0.6293, 0.5874, 0.5894, 0.5867, 0.5985, 0.5569, 0.5579, 0.5821, 0.5787, 0.5821, 0.6070, 0.5819, 0.5852, 0.6071, 0.5665, 0.6055, 0.5840, 0.5416]
Val_Loss: 0.608368, Val_Acc: 0.674757

Epoch #051, Train_Loss: [0.6331, 0.5081, 0.5624, 0.6248, 0.5973, 0.6130, 0.5453, 0.6078, 0.5607, 0.5861, 0.6046, 0.6056, 0.5913, 0.5161, 0.6034, 0.6180, 0.5863, 0.5287, 0.6090, 0.5793, 0.5862, 0.5747, 0.6232, 0.5991, 0.5741, 0.6000]
Val_Loss: 0.614536, Val_Acc: 0.686893

Epoch #052, Train_Loss: [0.6193, 0.5930, 0.5682, 0.5776, 0.5455, 0.6028, 0.6296, 0.5136, 0.5912, 0.5560, 0.5474, 0.5485, 0.6651, 0.5515, 0.5596, 0.5539, 0.5668, 0.5775, 0.6649, 0.6399, 0.5786, 0.5833, 0.5819, 0.5891, 0.4962, 0.6050]
Val_Loss: 0.610658, Val_Acc: 0.691748

Epoch #053, Train_Loss: [0.5624, 0.5605, 0.5475, 0.6406, 0.5354, 0.6618, 0.5867, 0.5218, 0.5304, 0.5469, 0.5876, 0.5439, 0.5189, 0.5965, 0.5650, 0.5851, 0.5857, 0.5453, 0.6413, 0.5799, 0.5746, 0.6597, 0.6192, 0.6590, 0.5494, 0.6122]
Val_Loss: 0.629330, Val_Acc: 0.652913

Epoch #054, Train_Loss: [0.5173, 0.5743, 0.6242, 0.5540, 0.6116, 0.5747, 0.6031, 0.6161, 0.5954, 0.5896, 0.5873, 0.5475, 0.5964, 0.5645, 0.6099, 0.7152, 0.5654, 0.6361, 0.5487, 0.5848, 0.5388, 0.6312, 0.5186, 0.5929, 0.5592, 0.5437]
Val_Loss: 0.608253, Val_Acc: 0.684466

Epoch #055, Train_Loss: [0.5606, 0.5767, 0.6066, 0.5483, 0.6111, 0.6389, 0.5948, 0.5497, 0.6026, 0.6020, 0.5294, 0.5094, 0.5428, 0.6301, 0.6073, 0.5437, 0.5701, 0.5554, 0.5220, 0.5909, 0.6130, 0.5694, 0.6119, 0.5545, 0.6071, 0.6242]
Val_Loss: 0.608011, Val_Acc: 0.684466

Epoch #056, Train_Loss: [0.5354, 0.5670, 0.6069, 0.5548, 0.5790, 0.6087, 0.5319, 0.5174, 0.6258, 0.5668, 0.5027, 0.5977, 0.5685, 0.5452, 0.5944, 0.5706, 0.5393, 0.5563, 0.5399, 0.5853, 0.5593, 0.6461, 0.5904, 0.6933, 0.6392, 0.6758]
Val_Loss: 0.608506, Val_Acc: 0.708738

Epoch #057, Train_Loss: [0.5795, 0.5915, 0.6208, 0.5702, 0.5983, 0.5531, 0.5984, 0.6018, 0.5507, 0.5690, 0.5326, 0.6281, 0.5752, 0.5554, 0.5797, 0.5393, 0.6803, 0.5745, 0.5399, 0.5900, 0.5533, 0.5607, 0.5252, 0.5609, 0.6018, 0.5623]
Val_Loss: 0.610220, Val_Acc: 0.677184

Epoch #058, Train_Loss: [0.5095, 0.5977, 0.5803, 0.5056, 0.5398, 0.5790, 0.5885, 0.6205, 0.6442, 0.5982, 0.5858, 0.6059, 0.5425, 0.5981, 0.5304, 0.5622, 0.6090, 0.6324, 0.5794, 0.6288, 0.5342, 0.5437, 0.5698, 0.5800, 0.6204, 0.5126]
Val_Loss: 0.607837, Val_Acc: 0.662621

Epoch #059, Train_Loss: [0.5372, 0.5781, 0.6424, 0.5855, 0.5393, 0.5937, 0.6136, 0.5460, 0.6029, 0.5532, 0.6187, 0.5568, 0.5805, 0.6226, 0.5507, 0.5877, 0.6498, 0.5811, 0.6300, 0.5906, 0.5767, 0.5570, 0.5799, 0.5768, 0.5797, 0.5259]
Val_Loss: 0.605384, Val_Acc: 0.679612

Epoch #060, Train_Loss: [0.6533, 0.5619, 0.5928, 0.6517, 0.5955, 0.5072, 0.5002, 0.5321, 0.5715, 0.5408, 0.6240, 0.6190, 0.5285, 0.5875, 0.5641, 0.5795, 0.5893, 0.5989, 0.5568, 0.5163, 0.5798, 0.5574, 0.5680, 0.5718, 0.5471, 0.5690]
Val_Loss: 0.605257, Val_Acc: 0.686893

Epoch #061, Train_Loss: [0.5577, 0.5770, 0.5301, 0.5844, 0.6042, 0.5456, 0.5406, 0.6131, 0.5860, 0.5816, 0.6069, 0.5886, 0.6099, 0.5550, 0.5448, 0.6360, 0.5604, 0.5498, 0.5753, 0.5588, 0.5084, 0.5273, 0.5989, 0.5476, 0.5763, 0.5828]
Val_Loss: 0.611611, Val_Acc: 0.669903

Epoch #062, Train_Loss: [0.5395, 0.5665, 0.5876, 0.6583, 0.6409, 0.6082, 0.6811, 0.6165, 0.5576, 0.5608, 0.5441, 0.5572, 0.5679, 0.6249, 0.5513, 0.4937, 0.5575, 0.6081, 0.6370, 0.5619, 0.5107, 0.5769, 0.5451, 0.5470, 0.5915, 0.5337]
Val_Loss: 0.605964, Val_Acc: 0.679612

Epoch #063, Train_Loss: [0.5288, 0.6014, 0.5567, 0.4895, 0.6020, 0.5956, 0.5293, 0.5946, 0.5122, 0.5524, 0.6251, 0.5900, 0.6017, 0.5584, 0.6076, 0.5482, 0.5954, 0.5587, 0.6535, 0.5389, 0.6230, 0.5542, 0.5502, 0.5442, 0.5907, 0.6453]
Val_Loss: 0.627496, Val_Acc: 0.652913

Epoch #064, Train_Loss: [0.5454, 0.6480, 0.6086, 0.5924, 0.5836, 0.5192, 0.5784, 0.6100, 0.5720, 0.5368, 0.5586, 0.5440, 0.5766, 0.5269, 0.5374, 0.6051, 0.5929, 0.5234, 0.6507, 0.6260, 0.6024, 0.5670, 0.5411, 0.5965, 0.5018, 0.5613]
Val_Loss: 0.599732, Val_Acc: 0.703883
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #065, Train_Loss: [0.5536, 0.5701, 0.5732, 0.5251, 0.5593, 0.5928, 0.5716, 0.5690, 0.5806, 0.5951, 0.4915, 0.6432, 0.5263, 0.5410, 0.5139, 0.5254, 0.6252, 0.5400, 0.5697, 0.5166, 0.6320, 0.6310, 0.6019, 0.5930, 0.5859, 0.5949]
Val_Loss: 0.603008, Val_Acc: 0.667476

Epoch #066, Train_Loss: [0.5661, 0.5914, 0.6020, 0.5048, 0.5843, 0.5824, 0.5602, 0.5264, 0.6040, 0.5757, 0.5496, 0.5909, 0.5754, 0.6167, 0.6191, 0.6286, 0.5330, 0.5857, 0.5365, 0.6085, 0.5226, 0.5805, 0.5636, 0.5757, 0.5289, 0.5458]
Val_Loss: 0.596311, Val_Acc: 0.701456
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #067, Train_Loss: [0.5858, 0.5469, 0.5004, 0.5558, 0.5947, 0.6139, 0.5805, 0.5661, 0.5529, 0.5863, 0.5552, 0.6069, 0.5937, 0.4826, 0.5751, 0.5798, 0.5486, 0.5312, 0.5374, 0.5926, 0.5889, 0.5750, 0.6064, 0.5333, 0.5858, 0.6472]
Val_Loss: 0.601487, Val_Acc: 0.696602

Epoch #068, Train_Loss: [0.5215, 0.5278, 0.6101, 0.6383, 0.5239, 0.5717, 0.5503, 0.6303, 0.5528, 0.6031, 0.6473, 0.5109, 0.5569, 0.5095, 0.4788, 0.5731, 0.6233, 0.5833, 0.5563, 0.6422, 0.5493, 0.5826, 0.4858, 0.5950, 0.6460, 0.5167]
Val_Loss: 0.606940, Val_Acc: 0.679612

Epoch #069, Train_Loss: [0.5800, 0.5740, 0.5041, 0.5388, 0.5734, 0.6368, 0.6051, 0.5155, 0.5689, 0.5490, 0.5551, 0.5996, 0.5469, 0.5600, 0.6307, 0.5571, 0.5647, 0.5582, 0.5677, 0.5895, 0.6593, 0.6651, 0.6404, 0.5504, 0.5418, 0.5636]
Val_Loss: 0.647848, Val_Acc: 0.623786

Epoch #070, Train_Loss: [0.6590, 0.5809, 0.6249, 0.5745, 0.5700, 0.5655, 0.6317, 0.5676, 0.5891, 0.6062, 0.6051, 0.5661, 0.5765, 0.5371, 0.5702, 0.5642, 0.4920, 0.5597, 0.6087, 0.5509, 0.5918, 0.5495, 0.5170, 0.5630, 0.5748, 0.5863]
Val_Loss: 0.603818, Val_Acc: 0.686893

Epoch #071, Train_Loss: [0.5365, 0.5736, 0.5530, 0.6177, 0.5908, 0.5487, 0.6098, 0.5516, 0.5962, 0.5925, 0.5873, 0.5476, 0.5748, 0.5817, 0.5953, 0.4848, 0.5634, 0.5751, 0.5443, 0.6703, 0.5671, 0.5799, 0.5160, 0.5468, 0.5479, 0.5984]
Val_Loss: 0.604309, Val_Acc: 0.672330

Epoch #072, Train_Loss: [0.5648, 0.6059, 0.5411, 0.6204, 0.5948, 0.6190, 0.5813, 0.5616, 0.5100, 0.5104, 0.5108, 0.5682, 0.5648, 0.6383, 0.5140, 0.5838, 0.5248, 0.5440, 0.5143, 0.6464, 0.5822, 0.6167, 0.5449, 0.6748, 0.6080, 0.5735]
Val_Loss: 0.609582, Val_Acc: 0.662621

Epoch #073, Train_Loss: [0.5786, 0.4848, 0.5916, 0.5689, 0.5792, 0.5128, 0.5607, 0.5232, 0.5348, 0.6016, 0.5851, 0.6090, 0.5774, 0.5703, 0.5733, 0.5994, 0.5285, 0.6349, 0.5881, 0.5542, 0.6158, 0.5650, 0.5946, 0.5480, 0.5758, 0.5845]
Val_Loss: 0.627556, Val_Acc: 0.648058

Epoch #074, Train_Loss: [0.6390, 0.5671, 0.5557, 0.5598, 0.5651, 0.5218, 0.5402, 0.6045, 0.5718, 0.5425, 0.5987, 0.5409, 0.5835, 0.6115, 0.5402, 0.5569, 0.5874, 0.5655, 0.5551, 0.5931, 0.5922, 0.5519, 0.5902, 0.6301, 0.5699, 0.5558]
Val_Loss: 0.600170, Val_Acc: 0.682039

Epoch #075, Train_Loss: [0.6574, 0.6369, 0.5292, 0.5761, 0.5502, 0.5477, 0.6188, 0.5648, 0.5810, 0.5586, 0.5425, 0.5747, 0.5651, 0.5532, 0.5406, 0.5434, 0.5910, 0.5279, 0.5532, 0.5842, 0.5078, 0.6252, 0.5628, 0.6302, 0.5450, 0.5903]
Val_Loss: 0.599038, Val_Acc: 0.701456

Epoch #076, Train_Loss: [0.5316, 0.5977, 0.5884, 0.5390, 0.5252, 0.5165, 0.6199, 0.5284, 0.5267, 0.6046, 0.6149, 0.4897, 0.5500, 0.5762, 0.6006, 0.5181, 0.4890, 0.5980, 0.5560, 0.5790, 0.5486, 0.6090, 0.5683, 0.5391, 0.5848, 0.5720]
Val_Loss: 0.600686, Val_Acc: 0.691748

Epoch #077, Train_Loss: [0.5305, 0.5182, 0.5385, 0.5296, 0.5631, 0.6264, 0.5219, 0.5603, 0.6083, 0.5444, 0.5851, 0.5563, 0.5642, 0.5493, 0.5769, 0.5391, 0.5703, 0.5539, 0.5685, 0.6231, 0.5889, 0.5828, 0.5270, 0.6077, 0.5459, 0.6211]
Val_Loss: 0.603901, Val_Acc: 0.699029

Epoch #078, Train_Loss: [0.6403, 0.5519, 0.5304, 0.5000, 0.5679, 0.5985, 0.5662, 0.5789, 0.5948, 0.5652, 0.5593, 0.5443, 0.5698, 0.6370, 0.5936, 0.5995, 0.5393, 0.4952, 0.5674, 0.5908, 0.6419, 0.5197, 0.5467, 0.5521, 0.4838, 0.5318]
Val_Loss: 0.617951, Val_Acc: 0.660194

Epoch #079, Train_Loss: [0.5633, 0.6238, 0.5732, 0.5602, 0.6716, 0.5542, 0.5685, 0.6041, 0.5014, 0.5466, 0.5766, 0.5238, 0.5275, 0.5309, 0.5360, 0.5603, 0.5253, 0.5783, 0.5639, 0.5050, 0.5934, 0.6010, 0.5684, 0.5840, 0.5427, 0.6140]
Val_Loss: 0.602209, Val_Acc: 0.689320

Epoch #080, Train_Loss: [0.4938, 0.5080, 0.5577, 0.4749, 0.5811, 0.6310, 0.5773, 0.5591, 0.5225, 0.5877, 0.6137, 0.6178, 0.5041, 0.5891, 0.5807, 0.5814, 0.5587, 0.5620, 0.5847, 0.6070, 0.5579, 0.5639, 0.5038, 0.5926, 0.5418, 0.6083]
Val_Loss: 0.597349, Val_Acc: 0.694175

Epoch #081, Train_Loss: [0.5652, 0.5335, 0.5716, 0.5201, 0.5055, 0.6315, 0.5982, 0.6439, 0.5987, 0.5196, 0.5655, 0.5369, 0.5473, 0.5527, 0.5843, 0.5031, 0.5991, 0.5853, 0.5960, 0.5307, 0.5654, 0.5303, 0.5908, 0.5856, 0.5727, 0.6293]
Val_Loss: 0.598195, Val_Acc: 0.682039

Epoch #082, Train_Loss: [0.5335, 0.5425, 0.5295, 0.5931, 0.5614, 0.5706, 0.5564, 0.5831, 0.5825, 0.5141, 0.5999, 0.5037, 0.5374, 0.5713, 0.5367, 0.5344, 0.5870, 0.5784, 0.5645, 0.5690, 0.6255, 0.5914, 0.6199, 0.5354, 0.5643, 0.5312]
Val_Loss: 0.596511, Val_Acc: 0.682039

Epoch #083, Train_Loss: [0.5939, 0.5810, 0.5372, 0.5673, 0.5532, 0.5789, 0.5519, 0.5429, 0.5233, 0.5807, 0.5789, 0.5798, 0.5585, 0.5131, 0.5677, 0.5950, 0.5081, 0.5762, 0.5747, 0.5454, 0.5777, 0.5501, 0.5525, 0.5882, 0.5221, 0.5760]
Val_Loss: 0.599906, Val_Acc: 0.708738

Epoch #084, Train_Loss: [0.5233, 0.5778, 0.5306, 0.5852, 0.6074, 0.5363, 0.6067, 0.5685, 0.5762, 0.5787, 0.5982, 0.5529, 0.5702, 0.5410, 0.6031, 0.4997, 0.5426, 0.5651, 0.6021, 0.5214, 0.5620, 0.5168, 0.5942, 0.5148, 0.5500, 0.5742]
Val_Loss: 0.600563, Val_Acc: 0.728155

Epoch #085, Train_Loss: [0.5140, 0.5248, 0.5988, 0.5076, 0.5922, 0.5386, 0.5282, 0.6772, 0.5721, 0.5834, 0.5377, 0.5386, 0.5958, 0.6257, 0.5997, 0.5605, 0.5474, 0.5732, 0.5663, 0.5984, 0.5776, 0.5338, 0.5662, 0.5989, 0.5055, 0.5571]
Val_Loss: 0.597834, Val_Acc: 0.711165

Epoch #086, Train_Loss: [0.5583, 0.5009, 0.6309, 0.5726, 0.5006, 0.4698, 0.4941, 0.5436, 0.5951, 0.5472, 0.6004, 0.5511, 0.5562, 0.6049, 0.5563, 0.5916, 0.5820, 0.5960, 0.5352, 0.5620, 0.6089, 0.5201, 0.5784, 0.5569, 0.5613, 0.5581]
Val_Loss: 0.594060, Val_Acc: 0.699029
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #087, Train_Loss: [0.5345, 0.6084, 0.5059, 0.5793, 0.5174, 0.5606, 0.6101, 0.5066, 0.6214, 0.5196, 0.6093, 0.5671, 0.5425, 0.5445, 0.5456, 0.5249, 0.4801, 0.5169, 0.5749, 0.6236, 0.5758, 0.6141, 0.5587, 0.5190, 0.5825, 0.6099]
Val_Loss: 0.601578, Val_Acc: 0.679612

Epoch #088, Train_Loss: [0.5367, 0.5303, 0.5689, 0.5405, 0.5372, 0.6154, 0.5479, 0.6085, 0.4945, 0.5492, 0.6064, 0.6136, 0.5117, 0.5051, 0.5007, 0.5318, 0.5852, 0.6104, 0.5753, 0.5689, 0.6295, 0.5423, 0.5338, 0.5829, 0.6030, 0.5434]
Val_Loss: 0.596142, Val_Acc: 0.711165

Epoch #089, Train_Loss: [0.5977, 0.5709, 0.5932, 0.5751, 0.6000, 0.6664, 0.5542, 0.5292, 0.6037, 0.5123, 0.5790, 0.5460, 0.4649, 0.5435, 0.5447, 0.5689, 0.5422, 0.4671, 0.5342, 0.5859, 0.6508, 0.5272, 0.5869, 0.5654, 0.4669, 0.4974]
Val_Loss: 0.598945, Val_Acc: 0.682039

Epoch #090, Train_Loss: [0.5672, 0.5086, 0.5517, 0.5731, 0.5720, 0.5788, 0.5020, 0.5558, 0.5464, 0.5496, 0.5979, 0.5404, 0.5628, 0.5592, 0.5044, 0.5542, 0.5508, 0.5272, 0.5651, 0.5144, 0.5682, 0.5589, 0.5282, 0.5647, 0.5346, 0.5959]
Val_Loss: 0.613502, Val_Acc: 0.682039

Epoch #091, Train_Loss: [0.5302, 0.5549, 0.5787, 0.5074, 0.5717, 0.5327, 0.6094, 0.6368, 0.5805, 0.5344, 0.6142, 0.5003, 0.5526, 0.4856, 0.5676, 0.5497, 0.5661, 0.5637, 0.5401, 0.5513, 0.5670, 0.5331, 0.5646, 0.5565, 0.5803, 0.6191]
Val_Loss: 0.599005, Val_Acc: 0.701456

Epoch #092, Train_Loss: [0.5320, 0.5517, 0.5491, 0.5352, 0.5500, 0.6464, 0.6123, 0.5897, 0.5113, 0.5661, 0.5692, 0.5778, 0.5720, 0.5712, 0.5956, 0.5114, 0.5809, 0.5207, 0.5526, 0.5437, 0.5344, 0.4859, 0.5257, 0.5346, 0.5085, 0.5069]
Val_Loss: 0.607172, Val_Acc: 0.689320

Epoch #093, Train_Loss: [0.5521, 0.5268, 0.5465, 0.5215, 0.6368, 0.6192, 0.5846, 0.5241, 0.4656, 0.5563, 0.5671, 0.6101, 0.6266, 0.6170, 0.4879, 0.5672, 0.5529, 0.5536, 0.5240, 0.5241, 0.5334, 0.5825, 0.5350, 0.5786, 0.4767, 0.6541]
Val_Loss: 0.595724, Val_Acc: 0.684466

Epoch #094, Train_Loss: [0.6817, 0.4871, 0.5529, 0.5407, 0.5426, 0.5473, 0.5724, 0.5624, 0.4937, 0.4976, 0.5174, 0.6626, 0.4916, 0.5628, 0.5389, 0.5305, 0.6357, 0.5057, 0.6404, 0.5112, 0.5176, 0.5042, 0.5816, 0.5749, 0.5198, 0.5286]
Val_Loss: 0.594155, Val_Acc: 0.686893

Epoch #095, Train_Loss: [0.5764, 0.5785, 0.5909, 0.5832, 0.5682, 0.5701, 0.5583, 0.5414, 0.5504, 0.4664, 0.5607, 0.5829, 0.5633, 0.5353, 0.5450, 0.5369, 0.5675, 0.5570, 0.5600, 0.5356, 0.5311, 0.5833, 0.4897, 0.5820, 0.6363, 0.5546]
Val_Loss: 0.590159, Val_Acc: 0.684466
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #096, Train_Loss: [0.6068, 0.5711, 0.5395, 0.5968, 0.5253, 0.5895, 0.5194, 0.5042, 0.5903, 0.5072, 0.5662, 0.5384, 0.5012, 0.5417, 0.5759, 0.5453, 0.5610, 0.5725, 0.6007, 0.5492, 0.5605, 0.5602, 0.5864, 0.5883, 0.5162, 0.5069]
Val_Loss: 0.604052, Val_Acc: 0.679612

Epoch #097, Train_Loss: [0.5048, 0.5388, 0.5464, 0.5543, 0.5465, 0.6335, 0.5135, 0.5761, 0.5767, 0.5388, 0.5410, 0.5460, 0.5427, 0.5510, 0.6104, 0.5276, 0.5548, 0.5295, 0.5847, 0.5390, 0.5772, 0.5628, 0.5032, 0.5344, 0.5477, 0.5479]
Val_Loss: 0.584839, Val_Acc: 0.699029
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #098, Train_Loss: [0.5611, 0.6034, 0.4985, 0.4648, 0.5734, 0.5377, 0.5845, 0.5268, 0.5095, 0.5774, 0.5854, 0.6413, 0.5348, 0.5620, 0.4886, 0.5922, 0.5852, 0.5243, 0.5277, 0.5695, 0.5747, 0.5721, 0.5576, 0.5170, 0.5326, 0.5039]
Val_Loss: 0.601977, Val_Acc: 0.677184

Epoch #099, Train_Loss: [0.5635, 0.5081, 0.5269, 0.5185, 0.5902, 0.4954, 0.5385, 0.5924, 0.5704, 0.5209, 0.5607, 0.5137, 0.5449, 0.4838, 0.6071, 0.5971, 0.5982, 0.5611, 0.6348, 0.4989, 0.5858, 0.5623, 0.5469, 0.5449, 0.5525, 0.5559]
Val_Loss: 0.597402, Val_Acc: 0.677184

Epoch #100, Train_Loss: [0.5068, 0.5564, 0.5448, 0.5250, 0.5694, 0.5426, 0.5624, 0.5271, 0.5663, 0.5662, 0.5268, 0.5151, 0.6009, 0.5433, 0.5469, 0.5724, 0.6264, 0.6189, 0.5806, 0.5272, 0.5998, 0.5688, 0.5850, 0.5762, 0.5442, 0.5713]
Val_Loss: 0.582906, Val_Acc: 0.691748
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #101, Train_Loss: [0.5615, 0.5414, 0.5542, 0.5904, 0.5848, 0.5147, 0.5393, 0.5145, 0.4911, 0.5614, 0.5430, 0.5844, 0.5150, 0.4823, 0.5941, 0.6172, 0.5616, 0.5988, 0.6208, 0.5913, 0.5396, 0.6191, 0.6227, 0.5508, 0.5050, 0.5524]
Val_Loss: 0.587192, Val_Acc: 0.689320

Epoch #102, Train_Loss: [0.5617, 0.5290, 0.6022, 0.5709, 0.5487, 0.5558, 0.5074, 0.5740, 0.5213, 0.5441, 0.5415, 0.6171, 0.5138, 0.5086, 0.5509, 0.5402, 0.5306, 0.5784, 0.5343, 0.5031, 0.5733, 0.5892, 0.6306, 0.5438, 0.6057, 0.5126]
Val_Loss: 0.582789, Val_Acc: 0.696602
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #103, Train_Loss: [0.5502, 0.5094, 0.5099, 0.5193, 0.5522, 0.5144, 0.5316, 0.5558, 0.5824, 0.5278, 0.5077, 0.5130, 0.5694, 0.5817, 0.5547, 0.6122, 0.5383, 0.5222, 0.5508, 0.5753, 0.5857, 0.6132, 0.5434, 0.5373, 0.5610, 0.5313]
Val_Loss: 0.594611, Val_Acc: 0.679612

Epoch #104, Train_Loss: [0.5384, 0.5435, 0.4954, 0.5163, 0.4794, 0.5216, 0.6161, 0.4971, 0.5677, 0.4950, 0.5469, 0.4908, 0.5360, 0.5237, 0.5118, 0.5877, 0.5407, 0.5644, 0.5760, 0.5785, 0.5298, 0.5995, 0.5346, 0.5525, 0.5660, 0.5542]
Val_Loss: 0.579623, Val_Acc: 0.691748
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #105, Train_Loss: [0.5671, 0.5184, 0.5447, 0.5134, 0.5451, 0.5498, 0.5034, 0.5196, 0.6047, 0.5464, 0.5488, 0.4983, 0.5679, 0.4920, 0.4980, 0.5640, 0.5775, 0.5876, 0.5001, 0.5760, 0.5343, 0.6125, 0.5971, 0.5639, 0.5382, 0.5559]
Val_Loss: 0.592133, Val_Acc: 0.686893

Epoch #106, Train_Loss: [0.5834, 0.4880, 0.5801, 0.5533, 0.5672, 0.5530, 0.5891, 0.5799, 0.5381, 0.5702, 0.5829, 0.5225, 0.5218, 0.5020, 0.5185, 0.5553, 0.4859, 0.5115, 0.5647, 0.4976, 0.5210, 0.5599, 0.5111, 0.5842, 0.5725, 0.5599]
Val_Loss: 0.592517, Val_Acc: 0.689320

Epoch #107, Train_Loss: [0.5392, 0.5477, 0.4675, 0.5622, 0.5842, 0.5640, 0.4805, 0.5374, 0.4922, 0.5738, 0.5403, 0.5452, 0.5817, 0.4924, 0.5232, 0.6157, 0.5655, 0.6281, 0.5366, 0.5285, 0.5763, 0.5476, 0.5934, 0.5446, 0.5397, 0.5414]
Val_Loss: 0.580283, Val_Acc: 0.696602

Epoch #108, Train_Loss: [0.5475, 0.5588, 0.5621, 0.5048, 0.5153, 0.5516, 0.4956, 0.5087, 0.5416, 0.5677, 0.4568, 0.5543, 0.5288, 0.6777, 0.5687, 0.5551, 0.6025, 0.4641, 0.5233, 0.6276, 0.5434, 0.5157, 0.6060, 0.4987, 0.5660, 0.5091]
Val_Loss: 0.573997, Val_Acc: 0.703883
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #109, Train_Loss: [0.5402, 0.5151, 0.5743, 0.4684, 0.5882, 0.5599, 0.6198, 0.5001, 0.5237, 0.5432, 0.5649, 0.5523, 0.4893, 0.5173, 0.5562, 0.5494, 0.5508, 0.5600, 0.5638, 0.5460, 0.5700, 0.5819, 0.5023, 0.5009, 0.5048, 0.5030]
Val_Loss: 0.593944, Val_Acc: 0.682039

Epoch #110, Train_Loss: [0.5869, 0.5014, 0.4995, 0.6194, 0.5212, 0.5724, 0.5545, 0.6062, 0.5629, 0.4971, 0.5193, 0.5635, 0.5383, 0.5451, 0.5797, 0.5424, 0.5090, 0.4868, 0.5369, 0.5373, 0.5219, 0.5394, 0.5046, 0.6192, 0.4830, 0.5334]
Val_Loss: 0.582527, Val_Acc: 0.691748

Epoch #111, Train_Loss: [0.5933, 0.5140, 0.5369, 0.6199, 0.5320, 0.5953, 0.5086, 0.4977, 0.5579, 0.5228, 0.5583, 0.5678, 0.5345, 0.4623, 0.5573, 0.5869, 0.5789, 0.4957, 0.5323, 0.5213, 0.6032, 0.5140, 0.5124, 0.5588, 0.5082, 0.5953]
Val_Loss: 0.584601, Val_Acc: 0.691748

Epoch #112, Train_Loss: [0.4389, 0.5221, 0.4912, 0.6035, 0.5390, 0.4974, 0.5068, 0.5537, 0.5263, 0.5042, 0.5833, 0.5125, 0.5338, 0.4817, 0.6220, 0.5587, 0.6442, 0.5818, 0.5596, 0.5452, 0.5627, 0.5242, 0.5479, 0.5239, 0.5033, 0.5394]
Val_Loss: 0.583677, Val_Acc: 0.691748

Epoch #113, Train_Loss: [0.5833, 0.5595, 0.5905, 0.5109, 0.5906, 0.5064, 0.5411, 0.5741, 0.4848, 0.5512, 0.6178, 0.5338, 0.5968, 0.5740, 0.5798, 0.5201, 0.5802, 0.6120, 0.5163, 0.5141, 0.5399, 0.5560, 0.5760, 0.5341, 0.5289, 0.4923]
Val_Loss: 0.583877, Val_Acc: 0.696602

Epoch #114, Train_Loss: [0.5681, 0.5111, 0.5253, 0.5874, 0.6377, 0.6133, 0.4779, 0.6002, 0.5812, 0.4770, 0.5363, 0.5608, 0.5116, 0.5472, 0.4999, 0.5129, 0.5389, 0.5372, 0.5710, 0.6098, 0.5160, 0.4733, 0.5327, 0.5585, 0.5229, 0.5574]
Val_Loss: 0.572794, Val_Acc: 0.701456
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #115, Train_Loss: [0.4917, 0.5118, 0.5187, 0.5181, 0.5666, 0.5343, 0.4965, 0.5010, 0.5583, 0.5701, 0.5745, 0.5376, 0.5550, 0.5752, 0.4919, 0.5088, 0.5546, 0.4767, 0.6171, 0.5447, 0.5884, 0.4997, 0.6035, 0.5207, 0.4950, 0.5588]
Val_Loss: 0.571515, Val_Acc: 0.703883
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #116, Train_Loss: [0.5116, 0.4606, 0.6055, 0.5216, 0.5810, 0.4794, 0.6685, 0.5677, 0.5124, 0.5790, 0.5755, 0.5003, 0.5456, 0.5458, 0.4862, 0.5503, 0.5483, 0.5751, 0.5632, 0.6085, 0.4609, 0.5402, 0.6374, 0.4992, 0.4858, 0.5952]
Val_Loss: 0.583133, Val_Acc: 0.694175

Epoch #117, Train_Loss: [0.4961, 0.6686, 0.4825, 0.4869, 0.5885, 0.5510, 0.4736, 0.4827, 0.5114, 0.6034, 0.5016, 0.4722, 0.5028, 0.4964, 0.5087, 0.5801, 0.5917, 0.5582, 0.4936, 0.5666, 0.5820, 0.5466, 0.5314, 0.4945, 0.6105, 0.5539]
Val_Loss: 0.584767, Val_Acc: 0.689320

Epoch #118, Train_Loss: [0.4813, 0.5772, 0.5293, 0.5613, 0.4864, 0.5942, 0.5823, 0.4311, 0.4954, 0.5272, 0.5290, 0.5532, 0.6268, 0.5675, 0.5236, 0.6348, 0.4576, 0.5351, 0.5173, 0.4965, 0.4938, 0.5394, 0.5801, 0.5726, 0.5906, 0.6235]
Val_Loss: 0.579501, Val_Acc: 0.694175

Epoch #119, Train_Loss: [0.5582, 0.5598, 0.5199, 0.4897, 0.5324, 0.5262, 0.5233, 0.5495, 0.4747, 0.5941, 0.5836, 0.5583, 0.5131, 0.4690, 0.4926, 0.5520, 0.5990, 0.6057, 0.5599, 0.5707, 0.5037, 0.6125, 0.5351, 0.5296, 0.5097, 0.4935]
Val_Loss: 0.579261, Val_Acc: 0.691748

Epoch #120, Train_Loss: [0.4943, 0.5144, 0.6079, 0.5859, 0.5025, 0.5449, 0.5583, 0.5195, 0.5585, 0.5475, 0.5173, 0.5308, 0.5367, 0.5643, 0.5712, 0.5647, 0.5573, 0.4378, 0.5382, 0.5455, 0.5155, 0.5031, 0.5269, 0.4683, 0.5914, 0.5351]
Val_Loss: 0.582789, Val_Acc: 0.699029

Epoch #121, Train_Loss: [0.5350, 0.4909, 0.5336, 0.4653, 0.5418, 0.5516, 0.4935, 0.4937, 0.4986, 0.5981, 0.5207, 0.4853, 0.5245, 0.5220, 0.5279, 0.5830, 0.5518, 0.6336, 0.5335, 0.5081, 0.6077, 0.5325, 0.4976, 0.4977, 0.6263, 0.5808]
Val_Loss: 0.576977, Val_Acc: 0.689320

Epoch #122, Train_Loss: [0.5901, 0.6026, 0.5465, 0.5556, 0.5118, 0.5018, 0.6033, 0.5348, 0.5182, 0.5066, 0.5341, 0.5820, 0.5476, 0.5581, 0.5281, 0.5036, 0.5217, 0.6521, 0.5506, 0.5316, 0.5041, 0.5604, 0.5899, 0.5328, 0.5017, 0.4741]
Val_Loss: 0.588098, Val_Acc: 0.686893

Epoch #123, Train_Loss: [0.5083, 0.6065, 0.5741, 0.5156, 0.4742, 0.5114, 0.5226, 0.4961, 0.5648, 0.5298, 0.5117, 0.5912, 0.5523, 0.5267, 0.4873, 0.5134, 0.5526, 0.5015, 0.5275, 0.5942, 0.5215, 0.6059, 0.5193, 0.5350, 0.5846, 0.4866]
Val_Loss: 0.580735, Val_Acc: 0.696602

Epoch #124, Train_Loss: [0.5499, 0.5834, 0.5526, 0.5613, 0.5737, 0.5590, 0.5092, 0.5455, 0.4921, 0.5282, 0.5220, 0.5455, 0.5715, 0.4542, 0.5500, 0.4528, 0.5420, 0.5137, 0.5579, 0.5758, 0.5269, 0.5118, 0.5721, 0.5143, 0.4253, 0.5676]
Val_Loss: 0.608774, Val_Acc: 0.667476

Epoch #125, Train_Loss: [0.5380, 0.5061, 0.5815, 0.4870, 0.5603, 0.6065, 0.5222, 0.4911, 0.5593, 0.5370, 0.5531, 0.5806, 0.5776, 0.4748, 0.4739, 0.4791, 0.4865, 0.5606, 0.6008, 0.5462, 0.5705, 0.6095, 0.5105, 0.5448, 0.5265, 0.5353]
Val_Loss: 0.579730, Val_Acc: 0.691748

Epoch #126, Train_Loss: [0.5014, 0.5340, 0.4766, 0.5836, 0.5500, 0.5216, 0.5114, 0.5229, 0.5037, 0.5216, 0.5273, 0.5073, 0.4681, 0.5760, 0.5698, 0.5564, 0.5858, 0.4501, 0.5460, 0.5524, 0.5924, 0.4948, 0.5414, 0.4612, 0.5904, 0.5477]
Val_Loss: 0.588645, Val_Acc: 0.691748

Epoch #127, Train_Loss: [0.5155, 0.6058, 0.4893, 0.5308, 0.5247, 0.5257, 0.5121, 0.5270, 0.5422, 0.5102, 0.5593, 0.5862, 0.5495, 0.5822, 0.5179, 0.5722, 0.5314, 0.5405, 0.5508, 0.5654, 0.5681, 0.5616, 0.5578, 0.5487, 0.4995, 0.5168]
Val_Loss: 0.583889, Val_Acc: 0.686893

Epoch #128, Train_Loss: [0.5166, 0.5322, 0.4952, 0.5443, 0.4907, 0.4894, 0.5073, 0.5364, 0.5195, 0.6051, 0.5162, 0.5448, 0.5224, 0.4993, 0.5891, 0.5325, 0.5358, 0.6454, 0.5565, 0.4659, 0.6067, 0.5469, 0.5092, 0.5717, 0.5179, 0.4431]
Val_Loss: 0.581252, Val_Acc: 0.684466

Epoch #129, Train_Loss: [0.5669, 0.5768, 0.5046, 0.5719, 0.6199, 0.5428, 0.5531, 0.5331, 0.4872, 0.6375, 0.5636, 0.5231, 0.5321, 0.5578, 0.5791, 0.5280, 0.5843, 0.4991, 0.5808, 0.6238, 0.4963, 0.5656, 0.5208, 0.5395, 0.4619, 0.4745]
Val_Loss: 0.597739, Val_Acc: 0.669903

Epoch #130, Train_Loss: [0.5463, 0.5221, 0.5882, 0.5316, 0.5684, 0.5568, 0.5339, 0.5224, 0.5428, 0.6039, 0.5158, 0.5488, 0.4762, 0.5198, 0.5342, 0.5259, 0.5333, 0.4787, 0.5390, 0.5290, 0.5190, 0.5231, 0.5025, 0.5439, 0.6162, 0.5494]
Val_Loss: 0.577010, Val_Acc: 0.706311

Epoch #131, Train_Loss: [0.4749, 0.5477, 0.6342, 0.4975, 0.4966, 0.5755, 0.5106, 0.5940, 0.5320, 0.4781, 0.4915, 0.5004, 0.5138, 0.5513, 0.5171, 0.5319, 0.5685, 0.5950, 0.5995, 0.5732, 0.5325, 0.5245, 0.5278, 0.5636, 0.4827, 0.4676]
Val_Loss: 0.579968, Val_Acc: 0.684466

Epoch #132, Train_Loss: [0.4592, 0.5455, 0.5079, 0.5143, 0.5641, 0.5592, 0.4965, 0.5318, 0.5346, 0.5022, 0.5354, 0.5414, 0.5897, 0.5981, 0.5153, 0.5658, 0.5305, 0.5298, 0.5839, 0.5303, 0.5831, 0.5431, 0.5083, 0.5024, 0.5142, 0.5262]
Val_Loss: 0.588117, Val_Acc: 0.679612

Epoch #133, Train_Loss: [0.5259, 0.5075, 0.4910, 0.4956, 0.5438, 0.5334, 0.5274, 0.5438, 0.5226, 0.4255, 0.5418, 0.5603, 0.4634, 0.5483, 0.5541, 0.5452, 0.5112, 0.4934, 0.5232, 0.5272, 0.5420, 0.5508, 0.4948, 0.5149, 0.5735, 0.5791]
Val_Loss: 0.581666, Val_Acc: 0.703883

Epoch #134, Train_Loss: [0.5607, 0.5114, 0.5347, 0.5628, 0.5138, 0.5358, 0.5836, 0.4853, 0.5174, 0.5341, 0.5899, 0.5418, 0.5119, 0.4861, 0.5251, 0.4736, 0.4424, 0.5554, 0.5870, 0.5977, 0.5097, 0.5301, 0.5203, 0.5444, 0.4832, 0.5991]
Val_Loss: 0.584401, Val_Acc: 0.689320

Epoch #135, Train_Loss: [0.5422, 0.5439, 0.5614, 0.5226, 0.4969, 0.5621, 0.4761, 0.5403, 0.5541, 0.5500, 0.4912, 0.5744, 0.5071, 0.5291, 0.4672, 0.5948, 0.5348, 0.5873, 0.5324, 0.4736, 0.4856, 0.4662, 0.5418, 0.5372, 0.4906, 0.5323]
Val_Loss: 0.596088, Val_Acc: 0.677184

Epoch #136, Train_Loss: [0.5926, 0.4714, 0.5222, 0.5085, 0.4806, 0.5179, 0.5302, 0.5972, 0.5610, 0.5061, 0.5194, 0.5386, 0.5928, 0.5056, 0.5015, 0.5061, 0.5133, 0.5272, 0.5295, 0.4708, 0.4978, 0.6505, 0.5024, 0.5650, 0.5174, 0.4881]
Val_Loss: 0.584900, Val_Acc: 0.691748

Epoch #137, Train_Loss: [0.5726, 0.5166, 0.5528, 0.5496, 0.4646, 0.4988, 0.5090, 0.5114, 0.5149, 0.5371, 0.5412, 0.5382, 0.4883, 0.5310, 0.4864, 0.4746, 0.5522, 0.5231, 0.5696, 0.5436, 0.5843, 0.4331, 0.4982, 0.5873, 0.5192, 0.5161]
Val_Loss: 0.586003, Val_Acc: 0.708738

Epoch #138, Train_Loss: [0.5664, 0.5181, 0.5998, 0.5404, 0.5476, 0.4607, 0.5768, 0.5661, 0.6077, 0.4826, 0.4705, 0.5479, 0.5147, 0.5905, 0.5847, 0.6068, 0.4900, 0.5073, 0.4589, 0.5245, 0.4904, 0.4951, 0.5297, 0.5546, 0.5084, 0.6289]
Val_Loss: 0.582081, Val_Acc: 0.701456

Epoch #139, Train_Loss: [0.5256, 0.5243, 0.5356, 0.5622, 0.5343, 0.5266, 0.4836, 0.5195, 0.4986, 0.5666, 0.5491, 0.4848, 0.4871, 0.5647, 0.6163, 0.5111, 0.5894, 0.4986, 0.5267, 0.5638, 0.5414, 0.5238, 0.5190, 0.4782, 0.5000, 0.5183]
Val_Loss: 0.570678, Val_Acc: 0.703883
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********

Epoch #140, Train_Loss: [0.5200, 0.5125, 0.4896, 0.4851, 0.4823, 0.5526, 0.5327, 0.5355, 0.5118, 0.4386, 0.5023, 0.5785, 0.5939, 0.4855, 0.5657, 0.5411, 0.6039, 0.5108, 0.5009, 0.5038, 0.4995, 0.5140, 0.5291, 0.5631, 0.5475, 0.5524]
Val_Loss: 0.587645, Val_Acc: 0.689320

Epoch #141, Train_Loss: [0.5219, 0.5564, 0.6312, 0.4980, 0.6036, 0.5139, 0.4945, 0.5174, 0.5599, 0.5013, 0.5372, 0.5703, 0.4810, 0.4547, 0.4620, 0.4681, 0.4904, 0.4978, 0.5207, 0.4766, 0.6030, 0.5082, 0.4949, 0.5844, 0.5489, 0.5681]
Val_Loss: 0.597228, Val_Acc: 0.706311

Epoch #142, Train_Loss: [0.5361, 0.4978, 0.5708, 0.6536, 0.4995, 0.5129, 0.5400, 0.5913, 0.5363, 0.5310, 0.5582, 0.5146, 0.5055, 0.5212, 0.5281, 0.6006, 0.5164, 0.4673, 0.4660, 0.6289, 0.4846, 0.5209, 0.4679, 0.5214, 0.5205, 0.4168]
Val_Loss: 0.591139, Val_Acc: 0.686893

Epoch #143, Train_Loss: [0.5205, 0.4987, 0.6154, 0.5343, 0.5825, 0.4594, 0.4949, 0.4838, 0.4717, 0.4861, 0.5954, 0.5260, 0.5843, 0.4966, 0.5251, 0.5400, 0.5419, 0.5174, 0.5058, 0.4722, 0.5919, 0.5723, 0.5550, 0.5216, 0.5466, 0.4912]
Val_Loss: 0.593782, Val_Acc: 0.691748

Epoch #144, Train_Loss: [0.5070, 0.5813, 0.5043, 0.5697, 0.5276, 0.4459, 0.5478, 0.4928, 0.4620, 0.4333, 0.5425, 0.6344, 0.5198, 0.5119, 0.5807, 0.5922, 0.4941, 0.5654, 0.5278, 0.5610, 0.4908, 0.5774, 0.4904, 0.4603, 0.5019, 0.5141]
Val_Loss: 0.575549, Val_Acc: 0.703883

Epoch #145, Train_Loss: [0.5348, 0.5169, 0.5215, 0.4993, 0.5234, 0.6101, 0.5921, 0.5133, 0.5239, 0.5376, 0.6136, 0.5059, 0.5354, 0.5289, 0.5319, 0.5212, 0.5275, 0.5572, 0.4995, 0.5525, 0.4957, 0.4736, 0.5436, 0.4753, 0.4891, 0.5753]
Val_Loss: 0.589697, Val_Acc: 0.691748

Epoch #146, Train_Loss: [0.4942, 0.5419, 0.6012, 0.5545, 0.5232, 0.5751, 0.4888, 0.4771, 0.4815, 0.5688, 0.5882, 0.4817, 0.4933, 0.5319, 0.5408, 0.5564, 0.4901, 0.5146, 0.5613, 0.4572, 0.5264, 0.5036, 0.5313, 0.5420, 0.5443, 0.5836]
Val_Loss: 0.582520, Val_Acc: 0.701456

Epoch #147, Train_Loss: [0.5428, 0.4789, 0.5121, 0.5242, 0.5401, 0.5074, 0.5412, 0.5037, 0.5562, 0.5339, 0.4535, 0.5334, 0.5790, 0.5645, 0.4953, 0.4910, 0.5005, 0.5631, 0.5020, 0.4770, 0.5178, 0.5067, 0.5339, 0.5373, 0.5112, 0.5213]
Val_Loss: 0.601360, Val_Acc: 0.667476

Epoch #148, Train_Loss: [0.5609, 0.5438, 0.5318, 0.4616, 0.4804, 0.4483, 0.5100, 0.4264, 0.5714, 0.4789, 0.4908, 0.4909, 0.6359, 0.4908, 0.4693, 0.5054, 0.5322, 0.5395, 0.5998, 0.5736, 0.5289, 0.5962, 0.5812, 0.5302, 0.5284, 0.5525]
Val_Loss: 0.582683, Val_Acc: 0.708738

Epoch #149, Train_Loss: [0.5142, 0.5366, 0.5373, 0.4733, 0.4900, 0.5832, 0.5363, 0.5032, 0.5377, 0.4419, 0.5362, 0.4221, 0.5461, 0.5413, 0.6037, 0.5697, 0.5576, 0.5070, 0.5400, 0.5768, 0.5524, 0.5263, 0.4951, 0.4418, 0.5381, 0.5623]
Val_Loss: 0.580388, Val_Acc: 0.696602

Epoch #150, Train_Loss: [0.5798, 0.4897, 0.4700, 0.5474, 0.5579, 0.5443, 0.5208, 0.5289, 0.5333, 0.5949, 0.5507, 0.5223, 0.4546, 0.5666, 0.5055, 0.5069, 0.4898, 0.5212, 0.5340, 0.5724, 0.5135, 0.4835, 0.5622, 0.5023, 0.5320, 0.5744]
Val_Loss: 0.608332, Val_Acc: 0.728155

Epoch #151, Train_Loss: [0.5820, 0.5250, 0.5117, 0.5185, 0.4732, 0.4855, 0.5448, 0.5431, 0.4307, 0.5018, 0.5140, 0.5804, 0.4714, 0.5379, 0.5157, 0.6086, 0.4299, 0.5371, 0.5154, 0.5067, 0.4974, 0.5495, 0.5519, 0.4998, 0.4834, 0.5530]
Val_Loss: 0.585471, Val_Acc: 0.706311

Epoch #152, Train_Loss: [0.5019, 0.4623, 0.5140, 0.6312, 0.5242, 0.5254, 0.5055, 0.4906, 0.5714, 0.5347, 0.5334, 0.5476, 0.5115, 0.5014, 0.5129, 0.4554, 0.5412, 0.4802, 0.5403, 0.4824, 0.5702, 0.5275, 0.4897, 0.5262, 0.4884, 0.4813]
Val_Loss: 0.594450, Val_Acc: 0.689320

Epoch #153, Train_Loss: [0.4883, 0.5197, 0.5267, 0.4450, 0.5655, 0.5637, 0.5182, 0.5437, 0.5197, 0.5051, 0.4612, 0.5204, 0.4970, 0.5053, 0.5441, 0.4839, 0.5090, 0.5664, 0.4770, 0.5155, 0.4804, 0.4995, 0.4809, 0.6071, 0.6231, 0.5660]
Val_Loss: 0.581430, Val_Acc: 0.716019

Epoch #154, Train_Loss: [0.4916, 0.5451, 0.5246, 0.4951, 0.5401, 0.5286, 0.5891, 0.4716, 0.5009, 0.5052, 0.4659, 0.5615, 0.4660, 0.4867, 0.5380, 0.5268, 0.5068, 0.4585, 0.4828, 0.5098, 0.6022, 0.4778, 0.6095, 0.5540, 0.5177, 0.6253]
Val_Loss: 0.584191, Val_Acc: 0.691748

Epoch #155, Train_Loss: [0.5025, 0.5374, 0.4818, 0.5321, 0.5028, 0.4789, 0.4764, 0.4905, 0.4343, 0.5017, 0.5509, 0.5259, 0.5400, 0.4613, 0.6579, 0.5243, 0.4812, 0.5437, 0.5188, 0.5322, 0.5872, 0.5547, 0.5732, 0.4945, 0.6188, 0.5152]
Val_Loss: 0.580008, Val_Acc: 0.708738

Epoch #156, Train_Loss: [0.5779, 0.5096, 0.4780, 0.5299, 0.4930, 0.5215, 0.5939, 0.4876, 0.5273, 0.5191, 0.5113, 0.5139, 0.5600, 0.4911, 0.5018, 0.4748, 0.4941, 0.5128, 0.4838, 0.5196, 0.5321, 0.4853, 0.5706, 3.6986, 3.5676, 2.6862]
Val_Loss: 2.449434, Val_Acc: 0.487864

Epoch #157, Train_Loss: [2.3385, 1.7703, 1.3529, 1.2539, 1.0903, 0.8092, 0.8234, 0.8061, 0.7996, 0.8042, 0.8281, 0.8670, 0.7823, 0.8237, 0.8278, 0.7761, 0.7717, 0.7909, 0.7249, 0.7298, 0.7135, 0.6957, 0.7061, 0.6956, 0.6930, 0.6954]
Val_Loss: 0.693312, Val_Acc: 0.509709

Epoch #158, Train_Loss: [0.6905, 0.6971, 0.6915, 0.6939, 0.6915, 0.6940, 0.6957, 0.6947, 0.6937, 0.6967, 0.6945, 0.6944, 0.6921, 0.6967, 0.6937, 0.6969, 0.6946, 0.6932, 0.6941, 0.6934, 0.6926, 0.6941, 0.6905, 0.6946, 0.6915, 0.6929]
Val_Loss: 0.693602, Val_Acc: 0.487864

Epoch #159, Train_Loss: [0.6949, 0.6928, 0.6915, 0.6917, 0.6929, 0.6937, 0.6921, 0.6945, 0.6930, 0.6930, 0.6928, 0.6930, 0.6931, 0.6922, 0.6929, 0.6940, 0.6931, 0.6939, 0.6937, 0.6920, 0.6946, 0.6934, 0.6935, 0.6937, 0.6918, 0.6925]
Val_Loss: 0.693390, Val_Acc: 0.487864

Epoch #160, Train_Loss: [0.6924, 0.6912, 0.6923, 0.6924, 0.6943, 0.6931, 0.6962, 0.6924, 0.6934, 0.6931, 0.6934, 0.6922, 0.6937, 0.6952, 0.6925, 0.6914, 0.6953, 0.6953, 0.6936, 0.6934, 0.6956, 0.6933, 0.6927, 0.6940, 0.6918, 0.6904]
Val_Loss: 0.693320, Val_Acc: 0.487864

Epoch #161, Train_Loss: [0.6928, 0.6944, 0.6926, 0.6926, 0.6934, 0.6916, 0.6927, 0.6936, 0.6934, 0.6949, 0.6946, 0.6937, 0.6933, 0.6934, 0.6923, 0.6940, 0.6927, 0.6944, 0.6930, 0.6942, 0.6922, 0.6930, 0.6927, 0.6928, 0.6950, 0.6934]
Val_Loss: 0.693249, Val_Acc: 0.487864

Epoch #162, Train_Loss: [0.6956, 0.6926, 0.6926, 0.6934, 0.6946, 0.6964, 0.6948, 0.6947, 0.6936, 0.6940, 0.6931, 0.6933, 0.6936, 0.6934, 0.6943, 0.6923, 0.6928, 0.6932, 0.6928, 0.6924, 0.6922, 0.6940, 0.6935, 0.6937, 0.6915, 0.6927]
Val_Loss: 0.693140, Val_Acc: 0.497573

Epoch #163, Train_Loss: [0.6933, 0.6940, 0.6926, 0.6924, 0.6934, 0.6934, 0.6932, 0.6928, 0.6942, 0.6937, 0.6908, 0.6939, 0.6913, 0.6928, 0.6955, 0.6942, 0.6945, 0.6930, 0.6936, 0.6941, 0.6933, 0.6939, 0.6924, 0.6926, 0.6910, 0.6949]
Val_Loss: 0.693281, Val_Acc: 0.487864

Epoch #164, Train_Loss: [0.6921, 0.6936, 0.6932, 0.6924, 0.6925, 0.6952, 0.6932, 0.6926, 0.6934, 0.6946, 0.6924, 0.6913, 0.6928, 0.6922, 0.6925, 0.6923, 0.6926, 0.6910, 0.6932, 0.6926, 0.6929, 0.6926, 0.6936, 0.6927, 0.6925, 0.6918]
Val_Loss: 0.693070, Val_Acc: 0.504854

Epoch #165, Train_Loss: [0.6931, 0.6912, 0.6924, 0.6960, 0.6933, 0.6926, 0.6922, 0.6920, 0.6929, 0.6928, 0.6946, 0.6923, 0.6931, 0.6936, 0.6932, 0.6923, 0.6947, 0.6938, 0.6921, 0.6940, 0.6931, 0.6928, 0.6918, 0.6934, 0.6930, 0.6934]
Val_Loss: 0.693078, Val_Acc: 0.500000

Epoch #166, Train_Loss: [0.6926, 0.6931, 0.6924, 0.6946, 0.6933, 0.6910, 0.6932, 0.6937, 0.6927, 0.6923, 0.6943, 0.6926, 0.6926, 0.6923, 0.6925, 0.6939, 0.6929, 0.6923, 0.6924, 0.6933, 0.6924, 0.6918, 0.6924, 0.6937, 0.6945, 0.6929]
Val_Loss: 0.693058, Val_Acc: 0.500000

Epoch #167, Train_Loss: [0.6935, 0.6933, 0.6933, 0.6926, 0.6933, 0.6922, 0.6923, 0.6933, 0.6933, 0.6921, 0.6918, 0.6932, 0.6931, 0.6934, 0.6937, 0.6935, 0.6930, 0.6923, 0.6915, 0.6928, 0.6915, 0.6900, 0.6930, 0.6927, 0.6928, 0.6919]
Val_Loss: 0.692904, Val_Acc: 0.565534

Epoch #168, Train_Loss: [0.6923, 0.6927, 0.6929, 0.6951, 0.6937, 0.6938, 0.6930, 0.6923, 0.6918, 0.6942, 0.6929, 0.6934, 0.6929, 0.6928, 0.6927, 0.6932, 0.6933, 0.6932, 0.6937, 0.6934, 0.6937, 0.6933, 0.6943, 0.6926, 0.6914, 0.6919]
Val_Loss: 0.692811, Val_Acc: 0.587379

Epoch #169, Train_Loss: [0.6928, 0.6940, 0.6934, 0.6948, 0.6928, 0.6914, 0.6932, 0.6933, 0.6928, 0.6934, 0.6920, 0.6923, 0.6949, 0.6927, 0.6904, 0.6936, 0.6922, 0.6934, 0.6930, 0.6934, 0.6937, 0.6940, 0.6927, 0.6946, 0.6932, 0.6914]
Val_Loss: 0.692850, Val_Acc: 0.533981

Epoch #170, Train_Loss: [0.6917, 0.6913, 0.6914, 0.6936, 0.6923, 0.6950, 0.6937, 0.6934, 0.6924, 0.6920, 0.6911, 0.6940, 0.6910, 0.6927, 0.6955, 0.6918, 0.6956, 0.6943, 0.6934, 0.6936, 0.6919, 0.6936, 0.6926, 0.6948, 0.6924, 0.6944]
Val_Loss: 0.692680, Val_Acc: 0.577670

Epoch #171, Train_Loss: [0.6935, 0.6932, 0.6919, 0.6923, 0.6936, 0.6947, 0.6932, 0.6923, 0.6923, 0.6933, 0.6935, 0.6909, 0.6905, 0.6908, 0.6934, 0.6943, 0.6921, 0.6921, 0.6920, 0.6954, 0.6932, 0.6927, 0.6930, 0.6931, 0.6932, 0.6930]
Val_Loss: 0.692733, Val_Acc: 0.512136

Epoch #172, Train_Loss: [0.6927, 0.6954, 0.6931, 0.6927, 0.6933, 0.6931, 0.6936, 0.6910, 0.6944, 0.6920, 0.6937, 0.6932, 0.6921, 0.6934, 0.6923, 0.6924, 0.6940, 0.6946, 0.6902, 0.6932, 0.6946, 0.6961, 0.6912, 0.6923, 0.6918, 0.6936]
Val_Loss: 0.692460, Val_Acc: 0.572816

Epoch #173, Train_Loss: [0.6929, 0.6910, 0.6919, 0.6899, 0.6929, 0.6901, 0.6939, 0.6934, 0.6941, 0.6929, 0.6941, 0.6933, 0.6914, 0.6928, 0.6952, 0.6924, 0.6940, 0.6945, 0.6944, 0.6922, 0.6934, 0.6896, 0.6937, 0.6954, 0.6934, 0.6955]
Val_Loss: 0.692679, Val_Acc: 0.480583

Epoch #174, Train_Loss: [0.6934, 0.6928, 0.6941, 0.6945, 0.6917, 0.6922, 0.6908, 0.6914, 0.6922, 0.6926, 0.6923, 0.6912, 0.6921, 0.6932, 0.6945, 0.6924, 0.6925, 0.6958, 0.6938, 0.6923, 0.6919, 0.6962, 0.6923, 0.6930, 0.6925, 0.6954]
Val_Loss: 0.692752, Val_Acc: 0.500000

Epoch #175, Train_Loss: [0.6940, 0.6923, 0.6902, 0.6928, 0.6945, 0.6903, 0.6940, 0.6934, 0.6926, 0.6916, 0.6941, 0.6912, 0.6914, 0.6923, 0.6907, 0.6934, 0.6934, 0.6897, 0.6934, 0.6933, 0.6920, 0.6923, 0.6922, 0.6937, 0.6908, 0.6913]
Val_Loss: 0.692415, Val_Acc: 0.560680

Epoch #176, Train_Loss: [0.6936, 0.6944, 0.6960, 0.6935, 0.6945, 0.6934, 0.6917, 0.6941, 0.6925, 0.6923, 0.6946, 0.6912, 0.6951, 0.6927, 0.6951, 0.6948, 0.6917, 0.6942, 0.6914, 0.6926, 0.6947, 0.6916, 0.6942, 0.6926, 0.6935, 0.6902]
Val_Loss: 0.692687, Val_Acc: 0.492718

Epoch #177, Train_Loss: [0.6908, 0.6909, 0.6953, 0.6930, 0.6912, 0.6899, 0.6923, 0.6950, 0.6930, 0.6938, 0.6909, 0.6934, 0.6897, 0.6900, 0.6937, 0.6949, 0.6972, 0.6940, 0.6929, 0.6936, 0.6971, 0.6924, 0.6932, 0.6948, 0.6920, 0.6937]
Val_Loss: 0.692558, Val_Acc: 0.483010

Epoch #178, Train_Loss: [0.6921, 0.6928, 0.6923, 0.6907, 0.6950, 0.6926, 0.6915, 0.6939, 0.6939, 0.6930, 0.6950, 0.6933, 0.6927, 0.6951, 0.6919, 0.6952, 0.6925, 0.6916, 0.6948, 0.6938, 0.6923, 0.6927, 0.6951, 0.6939, 0.6910, 0.6910]
Val_Loss: 0.692452, Val_Acc: 0.512136

Epoch #179, Train_Loss: [0.6946, 0.6918, 0.6896, 0.6900, 0.6925, 0.6916, 0.6973, 0.6908, 0.6896, 0.6930, 0.6928, 0.6924, 0.6914, 0.6943, 0.6898, 0.6924, 0.6912, 0.6922, 0.6924, 0.6922, 0.6899, 0.6915, 0.6950, 0.6961, 0.6932, 0.6930]
Val_Loss: 0.692191, Val_Acc: 0.541262

Epoch #180, Train_Loss: [0.6926, 0.6935, 0.6925, 0.6910, 0.6907, 0.6904, 0.6921, 0.6942, 0.6922, 0.6890, 0.6931, 0.6919, 0.6930, 0.6931, 0.6949, 0.6959, 0.6907, 0.6933, 0.6907, 0.6917, 0.6895, 0.6904, 0.6904, 0.6922, 0.6898, 0.6942]
Val_Loss: 0.692116, Val_Acc: 0.536408

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/NCI109_Hie/model.pth *********
TEST :: Test_Acc: 0.707729
