+-----------------+-------------------+
|    Parameter    |       Value       |
+=================+===================+
| Batch size      | 128               |
+-----------------+-------------------+
| Dataset         | NCI1              |
+-----------------+-------------------+
| Dropout ratio   | 0.5               |
+-----------------+-------------------+
| Epochs          | 10000             |
+-----------------+-------------------+
| Exp name        | NCI1_Glo          |
+-----------------+-------------------+
| Gpu index       | 0                 |
+-----------------+-------------------+
| Hid             | 128               |
+-----------------+-------------------+
| Lr              | 0.0005            |
+-----------------+-------------------+
| Model           | SAGPooling_Global |
+-----------------+-------------------+
| Patience        | 40                |
+-----------------+-------------------+
| Pooling ratio   | 0.5               |
+-----------------+-------------------+
| Seed            | 16                |
+-----------------+-------------------+
| Test batch size | 1                 |
+-----------------+-------------------+
| Weight decay    | 0.0001            |
+-----------------+-------------------+
Using GPU: 0
SAGPooling_Global(
  (conv1): GCNConv(37, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): SAGPooling(GCNConv, 384, ratio=0.5, multiplier=1.0)
  (lin1): Linear(in_features=768, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 145092
Using Adam

Epoch #000, Train_Loss: [0.6926, 0.6922, 0.6944, 0.6946, 0.6942, 0.6938, 0.6943, 0.6937, 0.6934, 0.6921, 0.6940, 0.6928, 0.6935, 0.6930, 0.6938, 0.6923, 0.6920, 0.6917, 0.6932, 0.6949, 0.6935, 0.6928, 0.6933, 0.6929, 0.6933, 0.6922]
Val_Loss: 0.692241, Val_Acc: 0.515815
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #001, Train_Loss: [0.6923, 0.6924, 0.6930, 0.6921, 0.6925, 0.6926, 0.6923, 0.6941, 0.6925, 0.6913, 0.6896, 0.6918, 0.6926, 0.6916, 0.6930, 0.6907, 0.6895, 0.6892, 0.6904, 0.6894, 0.6874, 0.6900, 0.6959, 0.6946, 0.6885, 0.6931]
Val_Loss: 0.686576, Val_Acc: 0.510949
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #002, Train_Loss: [0.6901, 0.6894, 0.6821, 0.6892, 0.6893, 0.6906, 0.6814, 0.6848, 0.6917, 0.6916, 0.6872, 0.6844, 0.6803, 0.6922, 0.6924, 0.6860, 0.6917, 0.6847, 0.6686, 0.6888, 0.6807, 0.6788, 0.6727, 0.6907, 0.6787, 0.6818]
Val_Loss: 0.673691, Val_Acc: 0.552311
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #003, Train_Loss: [0.6650, 0.6786, 0.7038, 0.6846, 0.6893, 0.6696, 0.7006, 0.6663, 0.7024, 0.6624, 0.6840, 0.6838, 0.6751, 0.6649, 0.6779, 0.6952, 0.6588, 0.6814, 0.6896, 0.6504, 0.6836, 0.6649, 0.6781, 0.6619, 0.6918, 0.6494]
Val_Loss: 0.665705, Val_Acc: 0.564477
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #004, Train_Loss: [0.6852, 0.6672, 0.6516, 0.7047, 0.6690, 0.6470, 0.6472, 0.6524, 0.6672, 0.6801, 0.6759, 0.6547, 0.6712, 0.6221, 0.7092, 0.6662, 0.6693, 0.6727, 0.6531, 0.6671, 0.6268, 0.6736, 0.6655, 0.6627, 0.6503, 0.6647]
Val_Loss: 0.665943, Val_Acc: 0.608273

Epoch #005, Train_Loss: [0.6534, 0.6846, 0.6560, 0.6631, 0.7060, 0.6506, 0.6113, 0.6575, 0.6689, 0.6460, 0.6292, 0.6469, 0.6942, 0.6861, 0.6852, 0.6608, 0.6074, 0.6844, 0.6912, 0.6541, 0.6616, 0.6566, 0.6837, 0.6269, 0.6269, 0.6862]
Val_Loss: 0.654697, Val_Acc: 0.596107
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #006, Train_Loss: [0.6654, 0.6759, 0.6495, 0.6589, 0.6756, 0.6509, 0.6338, 0.6087, 0.6827, 0.6283, 0.6332, 0.6704, 0.6507, 0.6308, 0.6298, 0.6499, 0.6779, 0.6534, 0.6538, 0.6073, 0.6389, 0.6887, 0.7035, 0.6281, 0.6758, 0.6303]
Val_Loss: 0.644123, Val_Acc: 0.613139
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #007, Train_Loss: [0.6499, 0.6298, 0.6666, 0.6987, 0.6303, 0.6948, 0.6557, 0.6328, 0.6241, 0.6651, 0.6220, 0.6103, 0.6528, 0.6404, 0.6665, 0.6193, 0.6443, 0.6569, 0.6551, 0.6296, 0.6763, 0.6646, 0.6677, 0.6303, 0.6297, 0.6003]
Val_Loss: 0.651810, Val_Acc: 0.627737

Epoch #008, Train_Loss: [0.6066, 0.6250, 0.6570, 0.6849, 0.6443, 0.6363, 0.6208, 0.6245, 0.6611, 0.7028, 0.6134, 0.6331, 0.6661, 0.6403, 0.6288, 0.6375, 0.6378, 0.6681, 0.6522, 0.6317, 0.5928, 0.6299, 0.6130, 0.6451, 0.6729, 0.6062]
Val_Loss: 0.641902, Val_Acc: 0.615572
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #009, Train_Loss: [0.6581, 0.5907, 0.6088, 0.6521, 0.6233, 0.6116, 0.6220, 0.6216, 0.6258, 0.6671, 0.6544, 0.6665, 0.6211, 0.6423, 0.5879, 0.6543, 0.6472, 0.6708, 0.6367, 0.6491, 0.5756, 0.5628, 0.6655, 0.6927, 0.6387, 0.5983]
Val_Loss: 0.629767, Val_Acc: 0.639903
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #010, Train_Loss: [0.5601, 0.6123, 0.6927, 0.5862, 0.6039, 0.6030, 0.6819, 0.6405, 0.6680, 0.6170, 0.6253, 0.6460, 0.6054, 0.5813, 0.6247, 0.6173, 0.6354, 0.6847, 0.6784, 0.6536, 0.6234, 0.6405, 0.6321, 0.6182, 0.6056, 0.6651]
Val_Loss: 0.632332, Val_Acc: 0.669100

Epoch #011, Train_Loss: [0.6126, 0.6326, 0.6304, 0.6129, 0.6190, 0.6170, 0.6365, 0.6067, 0.6257, 0.5997, 0.6262, 0.5870, 0.6126, 0.5910, 0.6030, 0.6474, 0.6325, 0.5755, 0.5941, 0.5830, 0.6630, 0.5986, 0.6115, 0.5828, 0.5987, 0.7673]
Val_Loss: 0.637786, Val_Acc: 0.635036

Epoch #012, Train_Loss: [0.6910, 0.5578, 0.6007, 0.5775, 0.5858, 0.6201, 0.6012, 0.6517, 0.6376, 0.6255, 0.5998, 0.5758, 0.6510, 0.6317, 0.5703, 0.6425, 0.6255, 0.5497, 0.6100, 0.6270, 0.5362, 0.5988, 0.6406, 0.6340, 0.6148, 0.6073]
Val_Loss: 0.622709, Val_Acc: 0.666667
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #013, Train_Loss: [0.5863, 0.6421, 0.6097, 0.6339, 0.6310, 0.5673, 0.5791, 0.6392, 0.5938, 0.6766, 0.5751, 0.6374, 0.6025, 0.5751, 0.5963, 0.6187, 0.6557, 0.5713, 0.5609, 0.5787, 0.5795, 0.5615, 0.6259, 0.5774, 0.5508, 0.6123]
Val_Loss: 0.615175, Val_Acc: 0.656934
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #014, Train_Loss: [0.5709, 0.6446, 0.6101, 0.6215, 0.6344, 0.5555, 0.5757, 0.6093, 0.6511, 0.6245, 0.5255, 0.6158, 0.6316, 0.6186, 0.6080, 0.6396, 0.6283, 0.5433, 0.5650, 0.5717, 0.6241, 0.6221, 0.6204, 0.5908, 0.5857, 0.6377]
Val_Loss: 0.628335, Val_Acc: 0.673966

Epoch #015, Train_Loss: [0.5485, 0.5992, 0.5866, 0.5678, 0.6123, 0.5846, 0.5795, 0.6154, 0.5843, 0.6298, 0.5659, 0.5739, 0.5959, 0.5955, 0.6372, 0.5850, 0.6318, 0.5651, 0.5515, 0.6320, 0.6302, 0.6719, 0.6439, 0.6283, 0.6026, 0.5949]
Val_Loss: 0.613459, Val_Acc: 0.683698
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #016, Train_Loss: [0.6115, 0.5401, 0.5761, 0.5735, 0.5749, 0.5960, 0.5897, 0.5861, 0.6443, 0.5954, 0.6386, 0.6020, 0.6160, 0.6095, 0.5350, 0.5295, 0.5826, 0.5926, 0.5548, 0.6087, 0.6031, 0.5959, 0.6173, 0.5744, 0.5606, 0.5804]
Val_Loss: 0.613436, Val_Acc: 0.698297
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #017, Train_Loss: [0.4702, 0.5824, 0.5864, 0.5693, 0.5337, 0.6692, 0.6231, 0.6235, 0.5766, 0.5446, 0.6124, 0.6195, 0.5555, 0.5549, 0.5482, 0.6107, 0.5781, 0.5646, 0.5453, 0.5850, 0.6886, 0.5902, 0.6082, 0.6351, 0.6407, 0.5759]
Val_Loss: 0.619521, Val_Acc: 0.683698

Epoch #018, Train_Loss: [0.5674, 0.6191, 0.6269, 0.5977, 0.6628, 0.5790, 0.5373, 0.5793, 0.6267, 0.5741, 0.5186, 0.6115, 0.5742, 0.5387, 0.5787, 0.6294, 0.5264, 0.5871, 0.6026, 0.6092, 0.5580, 0.5656, 0.5870, 0.5904, 0.6417, 0.5719]
Val_Loss: 0.628013, Val_Acc: 0.671533

Epoch #019, Train_Loss: [0.6219, 0.6091, 0.5844, 0.6146, 0.5981, 0.5986, 0.5125, 0.5949, 0.5913, 0.5552, 0.5919, 0.5846, 0.5542, 0.5890, 0.5355, 0.5689, 0.6363, 0.5382, 0.5426, 0.5612, 0.6428, 0.6147, 0.5316, 0.6296, 0.6660, 0.5717]
Val_Loss: 0.618070, Val_Acc: 0.686131

Epoch #020, Train_Loss: [0.6218, 0.5963, 0.5871, 0.5368, 0.6085, 0.6524, 0.6111, 0.5621, 0.5903, 0.5783, 0.5645, 0.6029, 0.5640, 0.5314, 0.5480, 0.6219, 0.5746, 0.6224, 0.5340, 0.5818, 0.5796, 0.5909, 0.5947, 0.5668, 0.6055, 0.5300]
Val_Loss: 0.620229, Val_Acc: 0.686131

Epoch #021, Train_Loss: [0.5715, 0.5939, 0.5966, 0.5487, 0.5060, 0.4977, 0.5405, 0.5973, 0.5559, 0.6415, 0.5449, 0.6291, 0.6469, 0.5294, 0.6347, 0.6984, 0.5237, 0.5719, 0.5619, 0.5297, 0.5872, 0.5123, 0.6542, 0.5742, 0.6176, 0.5408]
Val_Loss: 0.624517, Val_Acc: 0.676399

Epoch #022, Train_Loss: [0.5443, 0.5687, 0.6374, 0.5744, 0.5147, 0.5479, 0.5468, 0.6251, 0.5734, 0.6029, 0.5764, 0.5595, 0.5927, 0.5722, 0.5420, 0.5854, 0.5821, 0.5885, 0.5794, 0.5972, 0.6196, 0.5714, 0.6019, 0.5235, 0.5847, 0.5848]
Val_Loss: 0.610817, Val_Acc: 0.683698
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #023, Train_Loss: [0.5443, 0.5879, 0.5669, 0.5883, 0.6334, 0.5611, 0.5951, 0.5425, 0.5551, 0.6403, 0.5887, 0.6533, 0.6000, 0.5807, 0.5368, 0.5683, 0.5823, 0.5375, 0.5715, 0.5907, 0.5421, 0.5585, 0.6720, 0.5511, 0.6033, 0.6296]
Val_Loss: 0.630891, Val_Acc: 0.669100

Epoch #024, Train_Loss: [0.5722, 0.5647, 0.5868, 0.5558, 0.5834, 0.5673, 0.6056, 0.5445, 0.6739, 0.5212, 0.6401, 0.5415, 0.5658, 0.5543, 0.5929, 0.5318, 0.6142, 0.5674, 0.5205, 0.5999, 0.5674, 0.5398, 0.6096, 0.5899, 0.5695, 0.5831]
Val_Loss: 0.619363, Val_Acc: 0.693431

Epoch #025, Train_Loss: [0.5463, 0.5721, 0.5478, 1.0974, 0.9870, 0.9245, 0.8699, 0.8567, 0.8328, 0.7656, 0.7417, 0.7440, 0.7509, 0.7141, 0.7187, 0.6958, 0.7358, 0.6966, 0.6950, 0.7035, 0.6924, 0.6900, 0.7023, 0.6998, 0.6943, 0.7128]
Val_Loss: 0.693934, Val_Acc: 0.491484

Epoch #026, Train_Loss: [0.7074, 0.7050, 0.6876, 0.6806, 0.6822, 0.6789, 0.6896, 0.6830, 0.6878, 0.6727, 0.6877, 0.6784, 0.6734, 0.6660, 0.6877, 0.6684, 0.6971, 0.6531, 0.6692, 0.6885, 0.6903, 0.6780, 0.6826, 0.6720, 0.6725, 0.6680]
Val_Loss: 0.675392, Val_Acc: 0.583942

Epoch #027, Train_Loss: [0.6791, 0.6861, 0.6707, 0.6824, 0.6641, 0.6743, 0.6649, 0.6740, 0.6611, 0.6602, 0.6744, 0.6557, 0.6589, 0.6390, 0.6670, 0.6596, 0.6723, 0.6573, 0.6663, 0.6405, 0.6716, 0.6811, 0.6765, 0.6448, 0.6459, 0.6500]
Val_Loss: 0.665817, Val_Acc: 0.566910

Epoch #028, Train_Loss: [0.6660, 0.6580, 0.6698, 0.6447, 0.6489, 0.6505, 0.6507, 0.6737, 0.6629, 0.6542, 0.6255, 0.5810, 0.6490, 0.6657, 0.6288, 0.6439, 0.6633, 0.6296, 0.6231, 0.6475, 0.6470, 0.6299, 0.6074, 0.6524, 0.6907, 0.6047]
Val_Loss: 0.662292, Val_Acc: 0.579075

Epoch #029, Train_Loss: [0.6710, 0.7136, 0.6232, 0.6336, 0.6559, 0.5908, 0.6279, 0.6369, 0.6731, 0.6457, 0.6180, 0.6200, 0.6331, 0.6454, 0.6123, 0.5728, 0.6423, 0.7008, 0.6252, 0.6773, 0.6484, 0.6081, 0.6293, 0.6815, 0.5923, 0.6605]
Val_Loss: 0.662555, Val_Acc: 0.608273

Epoch #030, Train_Loss: [0.6482, 0.6538, 0.6581, 0.6742, 0.6551, 0.6470, 0.6109, 0.6092, 0.6366, 0.6266, 0.6235, 0.6297, 0.6151, 0.6263, 0.6990, 0.6003, 0.6312, 0.5845, 0.6307, 0.6180, 0.6383, 0.6319, 0.6595, 0.6207, 0.6385, 0.6189]
Val_Loss: 0.638675, Val_Acc: 0.647202

Epoch #031, Train_Loss: [0.6280, 0.6468, 0.6555, 0.6094, 0.6131, 0.5929, 0.6314, 0.6569, 0.6505, 0.6363, 0.6207, 0.6020, 0.6336, 0.6452, 0.6607, 0.5768, 0.6557, 0.6743, 0.6070, 0.6490, 0.6465, 0.6110, 0.6219, 0.6239, 0.6455, 0.5784]
Val_Loss: 0.634226, Val_Acc: 0.647202

Epoch #032, Train_Loss: [0.6372, 0.6506, 0.6364, 0.6635, 0.6306, 0.5672, 0.6628, 0.6475, 0.6156, 0.5851, 0.6407, 0.5981, 0.6345, 0.6596, 0.6401, 0.6092, 0.5740, 0.6250, 0.6082, 0.5972, 0.6436, 0.6215, 0.6540, 0.6143, 0.6331, 0.6485]
Val_Loss: 0.643295, Val_Acc: 0.618005

Epoch #033, Train_Loss: [0.6228, 0.6033, 0.5835, 0.6128, 0.6226, 0.6652, 0.6273, 0.6286, 0.6268, 0.6316, 0.5888, 0.7197, 0.6090, 0.6212, 0.6402, 0.5973, 0.6403, 0.6333, 0.5772, 0.6563, 0.6401, 0.6140, 0.6016, 0.6033, 0.6237, 0.5878]
Val_Loss: 0.631478, Val_Acc: 0.652068

Epoch #034, Train_Loss: [0.6758, 0.6078, 0.6170, 0.5765, 0.5935, 0.6526, 0.6268, 0.6149, 0.5866, 0.6227, 0.6561, 0.6026, 0.5949, 0.6314, 0.6916, 0.6416, 0.6099, 0.6182, 0.5727, 0.6581, 0.6326, 0.6204, 0.6337, 0.6456, 0.5971, 0.5970]
Val_Loss: 0.635409, Val_Acc: 0.639903

Epoch #035, Train_Loss: [0.5827, 0.5820, 0.6369, 0.5705, 0.6465, 0.6271, 0.6498, 0.6173, 0.5653, 0.6509, 0.6346, 0.6396, 0.6265, 0.6253, 0.6243, 0.6902, 0.6928, 0.6750, 0.6047, 0.5696, 0.6415, 0.6677, 0.5870, 0.6444, 0.6175, 0.6538]
Val_Loss: 0.631120, Val_Acc: 0.656934

Epoch #036, Train_Loss: [0.6214, 0.6272, 0.6173, 0.6290, 0.6224, 0.6442, 0.6398, 0.6615, 0.6281, 0.6033, 0.5946, 0.5700, 0.6355, 0.5911, 0.6092, 0.5712, 0.6088, 0.6071, 0.6630, 0.6175, 0.6699, 0.6111, 0.6396, 0.6523, 0.6273, 0.6094]
Val_Loss: 0.636728, Val_Acc: 0.639903

Epoch #037, Train_Loss: [0.6057, 0.6329, 0.6451, 0.6575, 0.6005, 0.5688, 0.6136, 0.6208, 0.6072, 0.6465, 0.6465, 0.5724, 0.6015, 0.6035, 0.6108, 0.5985, 0.6086, 0.5951, 0.6287, 0.6925, 0.6455, 0.6177, 0.6263, 0.6293, 0.6491, 0.6329]
Val_Loss: 0.667270, Val_Acc: 0.600973

Epoch #038, Train_Loss: [0.6521, 0.6634, 0.6683, 0.6147, 0.6390, 0.5974, 0.6532, 0.6412, 0.6176, 0.6477, 0.6574, 0.5976, 0.6008, 0.6413, 0.6578, 0.6200, 0.6511, 0.6097, 0.6010, 0.6189, 0.6049, 0.5972, 0.5811, 0.5925, 0.5931, 0.5742]
Val_Loss: 0.632282, Val_Acc: 0.661800

Epoch #039, Train_Loss: [0.5726, 0.6277, 0.6123, 0.6376, 0.6234, 0.5764, 0.6348, 0.6455, 0.6382, 0.5774, 0.5750, 0.6249, 0.6577, 0.6349, 0.6265, 0.6498, 0.5965, 0.6642, 0.6023, 0.5770, 0.5935, 0.5986, 0.6251, 0.6210, 0.6478, 0.5846]
Val_Loss: 0.632414, Val_Acc: 0.649635

Epoch #040, Train_Loss: [0.6931, 0.5869, 0.6176, 0.6816, 0.6235, 0.5687, 0.6563, 0.5881, 0.5961, 0.5789, 0.5482, 0.5743, 0.6158, 0.6570, 0.5258, 0.6475, 0.5933, 0.6245, 0.5391, 0.6906, 0.6362, 0.5955, 0.6300, 0.6704, 0.6435, 0.6075]
Val_Loss: 0.643217, Val_Acc: 0.632603

Epoch #041, Train_Loss: [0.6365, 0.6408, 0.6115, 0.5672, 0.6253, 0.6250, 0.5767, 0.6103, 0.6318, 0.6282, 0.5529, 0.5844, 0.6162, 0.5625, 0.5698, 0.6471, 0.6080, 0.6134, 0.6283, 0.5799, 0.6343, 0.5984, 0.6398, 0.6340, 0.6257, 0.5918]
Val_Loss: 0.633272, Val_Acc: 0.664234

Epoch #042, Train_Loss: [0.6416, 0.6692, 0.5662, 0.5874, 0.6066, 0.6322, 0.6311, 0.6341, 0.6079, 0.6233, 0.5860, 0.6178, 0.6016, 0.5938, 0.5792, 0.5923, 0.6252, 0.6082, 0.5806, 0.6108, 0.6552, 0.5788, 0.5950, 0.5854, 0.6013, 0.5859]
Val_Loss: 0.633187, Val_Acc: 0.647202

Epoch #043, Train_Loss: [0.6154, 0.6248, 0.6211, 0.5644, 0.6018, 0.6161, 0.6317, 0.5939, 0.6141, 0.6208, 0.6731, 1.8514, 1.5470, 0.9004, 0.8237, 0.7359, 0.7665, 0.7607, 0.7993, 0.8661, 0.7806, 0.8142, 0.7669, 0.7303, 0.6981, 0.7371]
Val_Loss: 0.728285, Val_Acc: 0.486618

Epoch #044, Train_Loss: [0.7339, 0.7262, 0.7305, 0.7405, 0.7087, 0.6963, 0.6992, 0.7140, 0.6895, 0.6644, 0.6890, 0.6821, 0.6919, 0.7039, 0.7045, 0.6929, 0.6926, 0.6913, 0.7006, 0.6899, 0.6862, 0.6974, 0.6935, 0.6807, 0.6868, 0.6919]
Val_Loss: 0.694934, Val_Acc: 0.486618

Epoch #045, Train_Loss: [0.6802, 0.6921, 0.6933, 0.6948, 0.6976, 0.6905, 0.6972, 0.6841, 0.6964, 0.6860, 0.6991, 0.6942, 0.6780, 0.6867, 0.6843, 0.6868, 0.6889, 0.6787, 0.6676, 0.6793, 0.6787, 0.6787, 0.6722, 0.6779, 0.6932, 0.7031]
Val_Loss: 0.688184, Val_Acc: 0.491484

Epoch #046, Train_Loss: [0.6825, 0.6751, 0.6740, 0.6785, 0.6877, 0.6887, 0.6808, 0.6791, 0.6846, 0.6935, 0.6728, 0.6785, 0.6875, 0.6810, 0.6714, 0.6663, 0.6908, 0.6715, 0.6905, 0.6769, 0.6683, 0.6735, 0.6704, 0.6890, 0.6715, 0.6994]
Val_Loss: 0.680986, Val_Acc: 0.562044

Epoch #047, Train_Loss: [0.6657, 0.6870, 0.6732, 0.6750, 0.6749, 0.7197, 0.6690, 0.6616, 0.6609, 0.6785, 0.6652, 0.6650, 0.6580, 0.6699, 0.6617, 0.6735, 0.6654, 0.6725, 0.6764, 0.6791, 0.6376, 0.6754, 0.6714, 0.6563, 0.6610, 0.6877]
Val_Loss: 0.667915, Val_Acc: 0.579075

Epoch #048, Train_Loss: [0.6715, 0.6454, 0.6613, 0.6652, 0.6551, 0.6514, 0.6609, 0.6722, 0.6729, 0.6467, 0.6553, 0.6489, 0.6641, 0.6452, 0.6342, 0.6235, 0.6579, 0.6571, 0.6460, 0.6804, 0.6591, 0.6576, 0.6349, 0.6547, 0.6219, 0.6377]
Val_Loss: 0.641266, Val_Acc: 0.605839

Epoch #049, Train_Loss: [0.6875, 0.6612, 0.6281, 0.6240, 0.6146, 0.6335, 0.5886, 0.5929, 0.6317, 0.6843, 0.6110, 0.6572, 0.6522, 0.6489, 0.6492, 0.6566, 0.7143, 0.6292, 0.6119, 0.6219, 0.5950, 0.6066, 0.6627, 0.6550, 0.6242, 0.6139]
Val_Loss: 0.628651, Val_Acc: 0.659367

Epoch #050, Train_Loss: [0.6360, 0.6331, 0.5946, 0.6469, 0.6207, 0.6405, 0.6055, 0.6218, 0.5786, 0.6488, 0.5946, 0.6099, 0.6603, 0.6047, 0.6051, 0.6374, 0.6075, 0.6034, 0.6325, 0.6420, 0.6678, 0.5848, 0.5714, 0.6412, 0.6524, 0.5835]
Val_Loss: 0.614319, Val_Acc: 0.673966

Epoch #051, Train_Loss: [0.6209, 0.6349, 0.5996, 0.5989, 0.5660, 0.6253, 0.5979, 0.6246, 0.6461, 0.5903, 0.6269, 0.5775, 0.6152, 0.6406, 0.5926, 0.6072, 0.6087, 0.6005, 0.6268, 0.6240, 0.5873, 0.6489, 0.5704, 0.6177, 0.5947, 0.6350]
Val_Loss: 0.635224, Val_Acc: 0.635036

Epoch #052, Train_Loss: [0.6683, 0.6319, 0.5781, 0.6099, 0.6393, 0.6038, 0.6018, 0.5618, 0.5938, 0.5996, 0.6235, 0.5999, 0.5647, 0.5753, 0.6961, 0.5888, 0.5687, 0.5898, 0.5903, 0.5803, 0.5816, 0.6088, 0.6204, 0.5668, 0.6286, 0.5386]
Val_Loss: 0.605595, Val_Acc: 0.681265
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********

Epoch #053, Train_Loss: [0.6339, 0.6560, 0.5777, 0.5722, 0.6163, 0.6409, 0.5893, 0.5921, 0.5788, 0.5634, 0.5970, 0.5563, 0.6437, 0.5751, 0.5939, 0.5857, 0.5655, 0.5020, 0.5583, 0.5884, 0.6251, 0.6263, 0.5237, 0.5569, 0.5315, 0.6700]
Val_Loss: 0.630579, Val_Acc: 0.686131

Epoch #054, Train_Loss: [0.5875, 0.6271, 0.5442, 0.5642, 0.5955, 0.5469, 0.5743, 0.5880, 0.5382, 0.6168, 0.5606, 0.6109, 0.6185, 0.6447, 0.5793, 0.5845, 0.5348, 0.5183, 0.6085, 0.5731, 0.5559, 0.6118, 0.5781, 0.6048, 0.5945, 0.6065]
Val_Loss: 0.912082, Val_Acc: 0.377129

Epoch #055, Train_Loss: [0.9442, 0.9018, 0.5575, 0.5941, 0.5734, 0.5966, 0.5839, 0.5709, 0.5773, 0.5936, 0.6789, 0.5698, 0.5940, 0.5564, 0.5655, 0.5482, 0.5606, 0.5744, 0.5661, 0.6112, 0.5794, 0.5655, 0.6560, 0.5943, 0.5846, 0.6023]
Val_Loss: 0.649379, Val_Acc: 0.661800

Epoch #056, Train_Loss: [0.6142, 0.5851, 0.5527, 0.6365, 0.5688, 0.5851, 0.5166, 0.5648, 0.6063, 0.5908, 0.6031, 0.5421, 0.5456, 0.5352, 0.5288, 0.5564, 0.6516, 0.6235, 0.5960, 0.5796, 0.5945, 0.5400, 0.5895, 0.6645, 0.6095, 0.6034]
Val_Loss: 0.635019, Val_Acc: 0.676399

Epoch #057, Train_Loss: [0.5479, 0.5924, 0.6244, 0.6487, 0.6132, 0.5698, 0.6129, 0.6061, 0.6254, 0.5938, 0.6095, 0.6407, 0.6122, 0.5805, 0.5356, 0.5433, 0.6032, 0.5656, 0.6153, 0.5655, 0.6086, 0.5723, 0.5731, 0.5645, 0.5487, 0.5715]
Val_Loss: 0.632919, Val_Acc: 0.695864

Epoch #058, Train_Loss: [0.6034, 0.5786, 0.6160, 0.6069, 0.5543, 0.5587, 0.5665, 0.5908, 0.5766, 0.5355, 0.5868, 0.5517, 0.6085, 0.5848, 0.5673, 0.6273, 0.5593, 0.5805, 0.5710, 0.5583, 0.5647, 0.5527, 0.5904, 0.5608, 0.4910, 0.5353]
Val_Loss: 0.632556, Val_Acc: 0.688564

Epoch #059, Train_Loss: [0.5934, 0.6411, 0.6826, 0.6367, 0.6321, 0.5645, 0.5362, 0.5254, 0.5555, 0.5353, 0.5434, 0.5641, 0.5684, 0.5428, 0.5585, 0.5484, 0.5841, 0.5468, 0.5781, 0.5220, 0.5373, 0.5045, 0.6171, 0.5350, 0.5805, 0.5682]
Val_Loss: 0.634743, Val_Acc: 0.698297

Epoch #060, Train_Loss: [0.5139, 0.5884, 0.6083, 0.5688, 0.5745, 0.6173, 0.5207, 0.5275, 0.5619, 0.5550, 0.5877, 0.5498, 0.5288, 0.5660, 0.5669, 0.5521, 0.5886, 0.6007, 0.5390, 0.4980, 0.5750, 0.5360, 0.5810, 0.5772, 0.5686, 0.6445]
Val_Loss: 0.648474, Val_Acc: 0.681265

Epoch #061, Train_Loss: [0.5217, 0.6091, 0.6144, 0.5273, 0.5346, 0.5352, 0.6129, 0.6360, 0.5976, 0.5186, 0.5107, 0.5314, 0.5708, 0.5701, 0.5946, 0.5753, 0.5647, 0.5793, 0.5367, 0.5423, 0.5244, 0.6013, 0.5447, 0.5474, 0.4998, 0.5943]
Val_Loss: 0.635847, Val_Acc: 0.683698

Epoch #062, Train_Loss: [0.5422, 0.6053, 0.6097, 0.5849, 0.5108, 0.6226, 0.5214, 0.5389, 0.6200, 0.5913, 0.5563, 0.5796, 0.5547, 0.5923, 0.5386, 0.5474, 0.5880, 0.5613, 0.4789, 0.4949, 0.5627, 0.5650, 0.6248, 0.5888, 0.5761, 0.5296]
Val_Loss: 0.639845, Val_Acc: 0.676399

Epoch #063, Train_Loss: [0.5623, 0.5926, 0.6120, 0.5947, 0.4574, 0.5878, 0.5015, 0.5382, 0.5578, 0.5428, 0.5586, 0.5362, 0.5666, 0.5886, 0.5309, 0.5743, 0.5457, 0.5876, 0.5590, 0.6119, 0.5525, 0.6018, 0.5250, 0.5535, 0.5898, 0.6354]
Val_Loss: 0.638753, Val_Acc: 0.688564

Epoch #064, Train_Loss: [0.5242, 0.5574, 0.5767, 0.5400, 0.6553, 0.5340, 0.5187, 0.5495, 0.6007, 0.4567, 0.5759, 0.5730, 0.4908, 0.6185, 0.5531, 0.6594, 0.5847, 0.5448, 0.5554, 0.5081, 0.5472, 0.5734, 0.5917, 0.5445, 0.5795, 0.5334]
Val_Loss: 0.656743, Val_Acc: 0.690998

Epoch #065, Train_Loss: [0.5116, 0.6782, 0.5317, 0.5239, 0.5754, 0.5225, 0.5103, 0.5563, 0.5872, 0.6137, 0.5412, 0.5086, 0.5443, 0.5478, 0.5748, 0.5294, 0.6545, 0.5127, 0.5923, 0.6052, 0.5139, 0.5355, 0.6353, 0.5636, 0.4918, 0.6370]
Val_Loss: 0.650269, Val_Acc: 0.705596

Epoch #066, Train_Loss: [0.5365, 0.5125, 0.5574, 0.4912, 0.5264, 0.5593, 0.5709, 0.5816, 0.5965, 0.6064, 0.5696, 0.5431, 0.6174, 0.5905, 0.5179, 0.5178, 0.5492, 0.5269, 0.5413, 0.5677, 0.6040, 0.6115, 0.5158, 0.5091, 0.5725, 0.5400]
Val_Loss: 0.641882, Val_Acc: 0.690998

Epoch #067, Train_Loss: [0.5581, 0.6407, 0.5852, 0.5210, 0.5389, 0.5356, 0.5039, 0.6388, 0.4988, 0.5778, 0.6111, 0.5713, 0.5992, 0.5067, 0.5624, 0.5521, 0.5574, 0.5270, 0.5448, 0.5929, 0.5350, 0.5450, 0.5832, 0.5367, 0.5606, 0.5234]
Val_Loss: 0.641842, Val_Acc: 0.703163

Epoch #068, Train_Loss: [0.5873, 0.5763, 0.5292, 0.5460, 0.6062, 0.5583, 0.5586, 0.5834, 0.5785, 0.5264, 0.5131, 0.5443, 0.5096, 0.5300, 0.5634, 0.5717, 0.5304, 0.5448, 0.5245, 0.5565, 0.5231, 0.6018, 0.5826, 0.5112, 0.5751, 0.4817]
Val_Loss: 0.651204, Val_Acc: 0.690998

Epoch #069, Train_Loss: [0.5426, 0.5740, 0.5903, 0.4900, 0.5719, 0.5369, 0.5305, 0.4989, 0.5267, 0.6162, 0.5740, 0.5576, 0.5414, 0.6094, 0.5539, 0.5140, 0.5529, 0.5893, 0.5791, 0.5568, 0.5225, 0.5182, 0.5959, 0.5157, 0.4721, 0.5285]
Val_Loss: 0.641835, Val_Acc: 0.690998

Epoch #070, Train_Loss: [0.4948, 0.5302, 0.5647, 0.5686, 0.4701, 0.5747, 0.5601, 0.4856, 0.5513, 0.5784, 0.6327, 0.5690, 0.5597, 0.6150, 0.5533, 0.5736, 0.5542, 0.5200, 0.5861, 0.5154, 0.5410, 0.5432, 0.4998, 0.5423, 0.5589, 0.5616]
Val_Loss: 0.649792, Val_Acc: 0.703163

Epoch #071, Train_Loss: [0.5105, 0.5407, 0.5730, 0.5721, 0.5709, 0.5053, 0.4980, 0.5832, 0.6062, 0.5397, 0.5216, 0.5145, 0.5976, 0.6085, 0.5494, 0.5080, 0.5314, 0.5422, 0.6183, 0.5786, 0.5696, 0.5833, 0.5882, 0.5815, 0.5350, 0.5771]
Val_Loss: 0.653018, Val_Acc: 0.686131

Epoch #072, Train_Loss: [0.5892, 0.4803, 0.5807, 0.5479, 0.5782, 0.6236, 0.5429, 0.5900, 0.6078, 0.5652, 0.5358, 0.4813, 0.6089, 0.5644, 0.5434, 0.5796, 0.5479, 0.5177, 0.5033, 0.5275, 0.5431, 0.5301, 0.5074, 0.5063, 0.5227, 0.5404]
Val_Loss: 0.656520, Val_Acc: 0.695864

Epoch #073, Train_Loss: [0.4392, 0.5849, 0.5502, 0.5489, 0.5460, 0.5610, 0.5106, 0.4840, 0.5909, 0.5038, 0.4787, 0.5214, 0.5720, 0.5791, 0.5401, 0.5584, 0.5429, 0.6137, 0.5261, 0.4368, 0.5476, 0.5201, 0.5384, 0.5725, 0.5633, 0.6255]
Val_Loss: 0.653208, Val_Acc: 0.703163

Epoch #074, Train_Loss: [0.4243, 0.5126, 0.5533, 0.5580, 0.4397, 0.5704, 0.6331, 0.5330, 0.5698, 0.5347, 0.5206, 0.5558, 0.5810, 0.4605, 0.5834, 0.5133, 0.5905, 0.5320, 0.5725, 0.5493, 0.5964, 0.5557, 0.5752, 0.5327, 0.5023, 0.6141]
Val_Loss: 0.650658, Val_Acc: 0.698297

Epoch #075, Train_Loss: [0.5514, 0.5060, 0.5042, 0.5384, 0.5217, 0.5168, 0.6221, 0.5716, 0.5224, 0.5263, 0.5477, 0.5236, 0.5492, 0.5222, 0.5937, 0.5628, 0.5377, 0.5689, 0.5257, 0.5247, 0.5407, 0.4925, 0.5660, 0.5482, 0.5730, 0.5626]
Val_Loss: 0.652579, Val_Acc: 0.705596

Epoch #076, Train_Loss: [0.5361, 0.5774, 0.5443, 0.5755, 0.4621, 0.5775, 0.5705, 0.5699, 0.5412, 0.4523, 0.5879, 0.5125, 0.4974, 0.8974, 0.7781, 0.8446, 0.7805, 0.8515, 0.7422, 0.7744, 0.7773, 0.6907, 0.7610, 0.6836, 0.7180, 0.6493]
Val_Loss: 0.689917, Val_Acc: 0.537713

Epoch #077, Train_Loss: [0.6687, 0.6966, 0.7101, 0.6950, 0.6612, 0.6522, 0.6791, 0.6595, 0.6596, 0.6608, 0.6705, 0.6495, 0.6570, 0.6637, 0.6581, 0.6712, 0.6661, 0.6515, 0.6567, 0.6263, 0.6484, 0.6450, 0.6448, 0.6417, 0.6311, 0.6430]
Val_Loss: 0.657910, Val_Acc: 0.581509

Epoch #078, Train_Loss: [0.6225, 0.6819, 0.6624, 0.6491, 0.6432, 0.6055, 0.6492, 0.6388, 0.6236, 0.6204, 0.6168, 0.6275, 0.6398, 0.6649, 0.6379, 0.6066, 0.6224, 0.5850, 0.6545, 0.6319, 0.6491, 0.6379, 0.6208, 0.6232, 0.6740, 0.6661]
Val_Loss: 0.649779, Val_Acc: 0.598540

Epoch #079, Train_Loss: [0.6117, 0.6358, 0.5972, 0.6663, 0.6052, 0.6133, 0.6545, 0.6603, 0.6408, 0.6016, 0.6438, 0.6528, 0.6783, 0.6229, 0.6674, 0.5815, 0.6462, 0.6295, 0.5904, 0.6496, 0.5924, 0.6061, 0.6361, 0.6239, 0.5926, 0.6140]
Val_Loss: 0.639992, Val_Acc: 0.620438

Epoch #080, Train_Loss: [0.6278, 0.6215, 0.6637, 0.6184, 0.6195, 0.5812, 0.5553, 0.6183, 0.6201, 0.5949, 0.5947, 0.6230, 0.6088, 0.6257, 0.6206, 0.6302, 0.6035, 0.6292, 0.5828, 0.6458, 0.6587, 0.6071, 0.6620, 0.6230, 0.6098, 0.6086]
Val_Loss: 0.634306, Val_Acc: 0.637470

Epoch #081, Train_Loss: [0.6224, 0.6319, 0.6209, 0.6815, 0.6203, 0.6072, 0.6555, 0.5657, 0.6200, 0.5955, 0.5689, 0.6282, 0.5995, 0.6513, 0.6782, 0.5806, 0.6540, 0.6266, 0.5991, 0.6107, 0.6344, 0.6186, 0.5987, 0.5906, 0.6186, 0.5560]
Val_Loss: 0.633038, Val_Acc: 0.635036

Epoch #082, Train_Loss: [0.6427, 0.6022, 0.6493, 0.6493, 0.6201, 0.6117, 0.6013, 0.6103, 0.6355, 0.5934, 0.6460, 0.5706, 0.5412, 0.6294, 0.6383, 0.6662, 0.6273, 0.5972, 0.6409, 0.7107, 0.5896, 0.5945, 0.6153, 0.5671, 0.5751, 0.6410]
Val_Loss: 0.639020, Val_Acc: 0.647202

Epoch #083, Train_Loss: [0.5773, 0.6272, 0.5914, 0.5985, 0.6316, 0.6098, 0.5878, 0.5928, 0.6205, 0.5910, 0.6171, 0.5923, 0.5447, 0.6010, 0.6123, 0.6879, 0.6428, 0.6097, 0.5987, 0.6480, 0.6432, 0.5870, 0.5967, 0.6015, 0.5739, 0.5724]
Val_Loss: 0.634450, Val_Acc: 0.652068

Epoch #084, Train_Loss: [0.6415, 0.6057, 0.6156, 0.5242, 0.5966, 0.5898, 0.6278, 0.6148, 0.6366, 0.6076, 0.5852, 0.6122, 0.5983, 0.5954, 0.6292, 0.5959, 0.6725, 0.6733, 0.6430, 0.5970, 0.5936, 0.5854, 0.6234, 0.6370, 0.5968, 0.5969]
Val_Loss: 0.633318, Val_Acc: 0.656934

Epoch #085, Train_Loss: [0.5837, 0.6388, 0.6096, 0.6380, 0.5826, 0.6874, 0.5916, 0.6008, 0.5696, 0.5935, 0.6075, 0.6496, 0.6099, 0.5936, 0.6499, 0.6449, 0.6252, 0.6389, 0.5843, 0.6287, 0.5743, 0.5879, 0.6265, 0.5769, 0.5619, 0.5938]
Val_Loss: 0.631847, Val_Acc: 0.649635

Epoch #086, Train_Loss: [0.5927, 0.5812, 0.6349, 0.6087, 0.6052, 0.5567, 0.6737, 0.6553, 0.6363, 0.6617, 0.6547, 0.5872, 0.6127, 0.6373, 0.6087, 0.6138, 0.6109, 0.6277, 0.6151, 0.5963, 0.5996, 0.6084, 0.6052, 0.5920, 0.6366, 0.6200]
Val_Loss: 0.633109, Val_Acc: 0.656934

Epoch #087, Train_Loss: [0.6160, 0.5714, 0.6180, 0.6562, 0.5614, 0.6580, 0.5973, 0.6353, 0.6394, 0.6049, 0.6264, 0.5488, 0.6289, 0.6423, 0.5788, 0.6099, 0.6141, 0.5460, 0.6275, 0.6406, 0.5915, 0.5491, 0.5926, 0.6106, 0.6251, 0.6762]
Val_Loss: 0.633339, Val_Acc: 0.669100

Epoch #088, Train_Loss: [0.5658, 0.6530, 0.6396, 0.5679, 0.5796, 0.5853, 0.6269, 0.6336, 0.6325, 0.5856, 0.6496, 0.6861, 0.6055, 0.6119, 0.5884, 0.6100, 0.5910, 0.6559, 0.5542, 0.6091, 0.5889, 0.5797, 0.5838, 0.6551, 0.5668, 0.5836]
Val_Loss: 0.628898, Val_Acc: 0.654501

Epoch #089, Train_Loss: [0.6215, 0.6039, 0.5982, 0.5850, 0.6279, 0.6196, 0.5975, 0.6102, 0.5839, 0.5736, 0.5588, 0.6021, 0.6133, 0.6528, 0.6150, 0.6257, 0.6361, 0.6320, 0.5708, 0.5818, 0.6409, 0.5887, 0.6300, 0.6231, 0.6079, 0.6100]
Val_Loss: 0.630200, Val_Acc: 0.659367

Epoch #090, Train_Loss: [0.6323, 0.5719, 0.6225, 0.6031, 0.6495, 0.6209, 0.5907, 0.5875, 0.6491, 0.6517, 0.6127, 0.5823, 0.6600, 0.6025, 0.5425, 0.5754, 0.5593, 0.5608, 0.6395, 0.6108, 0.6089, 0.6067, 0.6123, 0.6194, 0.6077, 0.6050]
Val_Loss: 0.641865, Val_Acc: 0.654501

Epoch #091, Train_Loss: [0.6157, 0.5879, 0.6257, 0.5574, 1.1891, 0.6047, 0.5799, 0.6555, 0.5869, 0.6025, 0.6065, 0.5837, 0.6056, 0.6484, 0.6871, 0.6135, 0.6336, 0.6433, 0.6568, 0.6327, 0.6167, 0.6463, 0.6415, 0.5777, 0.5839, 0.5912]
Val_Loss: 0.629617, Val_Acc: 0.654501

Epoch #092, Train_Loss: [0.6653, 0.6251, 0.6071, 0.5880, 0.6242, 0.6242, 0.5695, 0.5827, 0.6179, 0.6131, 0.5862, 0.5658, 0.5942, 0.6377, 0.5910, 0.6329, 0.6230, 0.5472, 0.5881, 0.6387, 0.6001, 0.5760, 0.5975, 0.6363, 0.5743, 0.6294]
Val_Loss: 0.630508, Val_Acc: 0.644769

Epoch #093, Train_Loss: [0.6362, 0.5878, 0.6057, 0.6474, 0.5710, 0.6173, 0.5802, 0.5708, 0.6053, 0.6006, 0.5833, 0.5968, 0.6325, 0.5803, 0.5974, 0.6294, 0.5742, 0.6247, 0.6368, 0.6260, 0.6306, 0.6167, 0.6341, 0.5943, 0.5560, 0.5764]
Val_Loss: 0.628674, Val_Acc: 0.661800

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/NCI1_Glo/model.pth *********
TEST :: Test_Acc: 0.690998
