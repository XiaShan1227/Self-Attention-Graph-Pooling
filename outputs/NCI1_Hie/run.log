+-----------------+-------------------------+
|    Parameter    |          Value          |
+=================+=========================+
| Batch size      | 128                     |
+-----------------+-------------------------+
| Dataset         | NCI1                    |
+-----------------+-------------------------+
| Dropout ratio   | 0.5                     |
+-----------------+-------------------------+
| Epochs          | 10000                   |
+-----------------+-------------------------+
| Exp name        | NCI1_Hie                |
+-----------------+-------------------------+
| Gpu index       | 0                       |
+-----------------+-------------------------+
| Hid             | 128                     |
+-----------------+-------------------------+
| Lr              | 0.0005                  |
+-----------------+-------------------------+
| Model           | SAGPooling_Hierarchical |
+-----------------+-------------------------+
| Patience        | 40                      |
+-----------------+-------------------------+
| Pooling ratio   | 0.5                     |
+-----------------+-------------------------+
| Seed            | 16                      |
+-----------------+-------------------------+
| Test batch size | 1                       |
+-----------------+-------------------------+
| Weight decay    | 0.0001                  |
+-----------------+-------------------------+
Using GPU: 0
SAGPooling_Hierarchical(
  (conv1): GCNConv(37, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): SAGPooling(GCNConv, 128, ratio=0.5, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 79300
Using Adam

Epoch #000, Train_Loss: [0.6928, 0.6960, 0.6955, 0.6948, 0.6985, 0.6942, 0.6897, 0.6980, 0.6940, 0.6920, 0.6963, 0.6936, 0.6919, 0.6908, 0.6922, 0.6934, 0.6934, 0.6925, 0.6921, 0.6912, 0.6966, 0.6934, 0.6965, 0.6914, 0.6949, 0.6909]
Val_Loss: 0.692741, Val_Acc: 0.515815
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #001, Train_Loss: [0.6940, 0.6933, 0.6958, 0.6930, 0.6967, 0.6926, 0.6932, 0.6918, 0.6932, 0.6929, 0.6950, 0.6930, 0.6931, 0.6926, 0.6928, 0.6929, 0.6937, 0.6926, 0.6920, 0.6927, 0.6926, 0.6933, 0.6938, 0.6923, 0.6926, 0.6930]
Val_Loss: 0.693009, Val_Acc: 0.515815

Epoch #002, Train_Loss: [0.6928, 0.6931, 0.6924, 0.6933, 0.6933, 0.6919, 0.6940, 0.6931, 0.6923, 0.6919, 0.6928, 0.6932, 0.6926, 0.6928, 0.6941, 0.6948, 0.6924, 0.6936, 0.6933, 0.6932, 0.6928, 0.6946, 0.6935, 0.6932, 0.6931, 0.6929]
Val_Loss: 0.692972, Val_Acc: 0.515815

Epoch #003, Train_Loss: [0.6936, 0.6926, 0.6922, 0.6926, 0.6929, 0.6916, 0.6931, 0.6922, 0.6933, 0.6929, 0.6942, 0.6939, 0.6937, 0.6913, 0.6932, 0.6943, 0.6937, 0.6941, 0.6924, 0.6924, 0.6941, 0.6921, 0.6934, 0.6939, 0.6923, 0.6937]
Val_Loss: 0.693150, Val_Acc: 0.472019

Epoch #004, Train_Loss: [0.6919, 0.6922, 0.6922, 0.6923, 0.6939, 0.6932, 0.6923, 0.6922, 0.6923, 0.6924, 0.6918, 0.6915, 0.6922, 0.6922, 0.6935, 0.6951, 0.6907, 0.6938, 0.6933, 0.6937, 0.6914, 0.6940, 0.6916, 0.6926, 0.6926, 0.6937]
Val_Loss: 0.692735, Val_Acc: 0.530414
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #005, Train_Loss: [0.6917, 0.6926, 0.6916, 0.6919, 0.6924, 0.6911, 0.6911, 0.6966, 0.6916, 0.6957, 0.6938, 0.6932, 0.6906, 0.6914, 0.6903, 0.6904, 0.6924, 0.6922, 0.6917, 0.6943, 0.6922, 0.6913, 0.6901, 0.6914, 0.6888, 0.6948]
Val_Loss: 0.692279, Val_Acc: 0.508516
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #006, Train_Loss: [0.6938, 0.6911, 0.6927, 0.6927, 0.6899, 0.6915, 0.6909, 0.6884, 0.6879, 0.6936, 0.6915, 0.6881, 0.6908, 0.6881, 0.6943, 0.6906, 0.6874, 0.6894, 0.6898, 0.6910, 0.6879, 0.6875, 0.6881, 0.6898, 0.6942, 0.6948]
Val_Loss: 0.690285, Val_Acc: 0.552311
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #007, Train_Loss: [0.6840, 0.6882, 0.6896, 0.6852, 0.6872, 0.6892, 0.6904, 0.6785, 0.6823, 0.6891, 0.6951, 0.6890, 0.6814, 0.6837, 0.6970, 0.6900, 0.6786, 0.6782, 0.6879, 0.6801, 0.6888, 0.6741, 0.6922, 0.6690, 0.6678, 0.6873]
Val_Loss: 0.691820, Val_Acc: 0.530414

Epoch #008, Train_Loss: [0.7059, 0.6845, 0.6703, 0.6712, 0.6882, 0.6894, 0.6768, 0.6957, 0.6717, 0.6901, 0.6938, 0.6857, 0.6720, 0.6900, 0.6669, 0.6761, 0.6576, 0.6815, 0.6878, 0.6664, 0.6931, 0.6762, 0.6738, 0.6988, 0.6759, 0.6835]
Val_Loss: 0.689426, Val_Acc: 0.525547
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #009, Train_Loss: [0.6699, 0.6865, 0.6997, 0.6723, 0.6706, 0.6489, 0.6841, 0.6874, 0.6866, 0.6743, 0.6637, 0.6766, 0.6898, 0.6696, 0.6833, 0.6479, 0.6908, 0.6794, 0.6787, 0.6879, 0.6795, 0.6717, 0.6623, 0.6970, 0.6606, 0.6791]
Val_Loss: 0.687514, Val_Acc: 0.549878
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #010, Train_Loss: [0.6864, 0.6698, 0.6891, 0.6509, 0.6672, 0.6867, 0.7001, 0.6919, 0.6762, 0.6573, 0.6806, 0.6611, 0.6398, 0.6557, 0.6542, 0.6949, 0.6675, 0.6806, 0.6744, 0.6846, 0.6503, 0.6813, 0.6755, 0.6369, 0.6898, 0.7033]
Val_Loss: 0.686587, Val_Acc: 0.559611
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #011, Train_Loss: [0.6523, 0.6711, 0.6331, 0.6503, 0.7015, 0.7024, 0.6619, 0.6738, 0.6528, 0.6608, 0.6704, 0.6795, 0.6979, 0.6782, 0.6817, 0.6747, 0.6745, 0.6565, 0.6703, 0.6430, 0.6705, 0.6647, 0.6331, 0.6587, 0.6849, 0.6764]
Val_Loss: 0.683961, Val_Acc: 0.562044
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #012, Train_Loss: [0.6638, 0.6461, 0.6542, 0.6975, 0.6401, 0.6642, 0.6626, 0.6816, 0.6790, 0.6497, 0.6802, 0.6729, 0.6364, 0.6862, 0.6327, 0.6546, 0.6897, 0.6688, 0.6855, 0.6752, 0.6429, 0.6596, 0.6752, 0.6651, 0.6802, 0.6819]
Val_Loss: 0.688049, Val_Acc: 0.562044

Epoch #013, Train_Loss: [0.7010, 0.6758, 0.6930, 0.6700, 0.6320, 0.6588, 0.6786, 0.6625, 0.6699, 0.6555, 0.6567, 0.6615, 0.6511, 0.6760, 0.6752, 0.6800, 0.6825, 0.6466, 0.6580, 0.6640, 0.6519, 0.6755, 0.6857, 0.6483, 0.6639, 0.6648]
Val_Loss: 0.682598, Val_Acc: 0.564477
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #014, Train_Loss: [0.6613, 0.6995, 0.6924, 0.6574, 0.6570, 0.6567, 0.6664, 0.6442, 0.6542, 0.6229, 0.6744, 0.6578, 0.6545, 0.6548, 0.6959, 0.6469, 0.6500, 0.6349, 0.6942, 0.6880, 0.6513, 0.6715, 0.6845, 0.6700, 0.6395, 0.6901]
Val_Loss: 0.680696, Val_Acc: 0.581509
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #015, Train_Loss: [0.6225, 0.6231, 0.6732, 0.6795, 0.6193, 0.6798, 0.6239, 0.6757, 0.6971, 0.6552, 0.6720, 0.6523, 0.6679, 0.6782, 0.6622, 0.6562, 0.7023, 0.6956, 0.6456, 0.6992, 0.6597, 0.6314, 0.6387, 0.6625, 0.7034, 0.6630]
Val_Loss: 0.677869, Val_Acc: 0.554745
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #016, Train_Loss: [0.6737, 0.6570, 0.6566, 0.6596, 0.6575, 0.6720, 0.6522, 0.6730, 0.6895, 0.6504, 0.6666, 0.7005, 0.6669, 0.6641, 0.6610, 0.6755, 0.6764, 0.6665, 0.6504, 0.6754, 0.6653, 0.6645, 0.6390, 0.6401, 0.6482, 0.6543]
Val_Loss: 0.677227, Val_Acc: 0.579075
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #017, Train_Loss: [0.6312, 0.6331, 0.6726, 0.6610, 0.6574, 0.6484, 0.6716, 0.6538, 0.6672, 0.6661, 0.6675, 0.6693, 0.6680, 0.6698, 0.7045, 0.6408, 0.6953, 0.7074, 0.6318, 0.6820, 0.6703, 0.6727, 0.6255, 0.6671, 0.6498, 0.6313]
Val_Loss: 0.678545, Val_Acc: 0.566910

Epoch #018, Train_Loss: [0.6621, 0.7019, 0.6363, 0.6310, 0.6433, 0.6523, 0.6295, 0.6671, 0.6209, 0.6589, 0.6848, 0.6561, 0.6444, 0.6507, 0.6011, 0.6446, 0.7010, 0.7027, 0.6535, 0.6269, 0.6624, 0.6888, 0.7017, 0.6259, 0.6277, 0.6198]
Val_Loss: 0.677694, Val_Acc: 0.576642

Epoch #019, Train_Loss: [0.6275, 0.6691, 0.6307, 0.6738, 0.6115, 0.6304, 0.6439, 0.6072, 0.6630, 0.6379, 0.7296, 0.6330, 0.6251, 0.6635, 0.6683, 0.6821, 0.6662, 0.6479, 0.6398, 0.6581, 0.6612, 0.6283, 0.6395, 0.6304, 0.6510, 0.6800]
Val_Loss: 0.678385, Val_Acc: 0.583942

Epoch #020, Train_Loss: [0.6301, 0.6202, 0.6489, 0.6934, 0.6235, 0.6499, 0.6245, 0.6195, 0.6948, 0.6228, 0.6572, 0.6714, 0.6314, 0.6790, 0.6581, 0.6320, 0.6778, 0.6503, 0.6817, 0.6213, 0.6692, 0.6524, 0.6328, 0.6592, 0.6298, 0.6114]
Val_Loss: 0.685396, Val_Acc: 0.583942

Epoch #021, Train_Loss: [0.6385, 0.6487, 0.6619, 0.6414, 0.6650, 0.6757, 0.6500, 0.6648, 0.6582, 0.6235, 0.6516, 0.6547, 0.6501, 0.6498, 0.6464, 0.6847, 0.6546, 0.6332, 0.6134, 0.6568, 0.6406, 0.6198, 0.6569, 0.6311, 0.6411, 0.6634]
Val_Loss: 0.677067, Val_Acc: 0.596107
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #022, Train_Loss: [0.6292, 0.6235, 0.6640, 0.6636, 0.6821, 0.7144, 0.6254, 0.6663, 0.6162, 0.6309, 0.6306, 0.6547, 0.6644, 0.6544, 0.6344, 0.6399, 0.6315, 0.6479, 0.6768, 0.6366, 0.6357, 0.6583, 0.6151, 0.6427, 0.6385, 0.6650]
Val_Loss: 0.677065, Val_Acc: 0.593674
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #023, Train_Loss: [0.6509, 0.6817, 0.6331, 0.5969, 0.6561, 0.6330, 0.5946, 0.6221, 0.7029, 0.6500, 0.6560, 0.5913, 0.6686, 0.6198, 0.6621, 0.6527, 0.6078, 0.6568, 0.6129, 0.6565, 0.6627, 0.6426, 0.6490, 0.6189, 0.6645, 0.6501]
Val_Loss: 0.684160, Val_Acc: 0.588808

Epoch #024, Train_Loss: [0.6460, 0.6105, 0.6433, 0.6240, 0.6826, 0.6344, 0.6727, 0.5981, 0.6226, 0.6608, 0.6140, 0.6435, 0.6662, 0.6222, 0.6990, 0.6468, 0.6472, 0.6336, 0.6221, 0.6670, 0.5937, 0.5912, 0.6471, 0.6436, 0.6689, 0.6416]
Val_Loss: 0.688560, Val_Acc: 0.576642

Epoch #025, Train_Loss: [0.6486, 0.6109, 0.6106, 0.6231, 0.6683, 0.5984, 0.6185, 0.6549, 0.6393, 0.6711, 0.6307, 0.6214, 0.6477, 0.6122, 0.6502, 0.5877, 0.6826, 0.6833, 0.6300, 0.6339, 0.6404, 0.6413, 0.6139, 0.6466, 0.6389, 0.6198]
Val_Loss: 0.677510, Val_Acc: 0.586375

Epoch #026, Train_Loss: [0.6253, 0.6513, 0.6610, 0.6196, 0.5904, 0.6241, 0.6692, 0.6375, 0.6572, 0.6084, 0.6570, 0.5980, 0.6083, 0.5774, 0.6073, 0.6455, 0.6279, 0.6325, 0.5852, 0.6506, 0.5922, 0.6180, 0.6523, 0.6786, 0.6548, 0.6837]
Val_Loss: 0.696249, Val_Acc: 0.576642

Epoch #027, Train_Loss: [0.6638, 0.5911, 0.6579, 0.6896, 0.6586, 0.6394, 0.6248, 0.6637, 0.6511, 0.6397, 0.6203, 0.6032, 0.6101, 0.6383, 0.6511, 0.6705, 0.6339, 0.6580, 0.6364, 0.6220, 0.6036, 0.6582, 0.6360, 0.6273, 0.6822, 0.6544]
Val_Loss: 0.677911, Val_Acc: 0.596107

Epoch #028, Train_Loss: [0.6183, 0.6080, 0.6188, 0.5998, 0.6428, 0.6311, 0.6467, 0.6233, 0.6432, 0.6399, 0.6066, 0.6235, 0.6338, 0.6117, 0.6851, 0.6342, 0.6010, 0.6307, 0.6574, 0.6317, 0.6099, 0.6276, 0.6309, 0.6207, 0.6164, 0.6008]
Val_Loss: 0.676550, Val_Acc: 0.598540
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #029, Train_Loss: [0.6423, 0.6188, 0.6596, 0.6636, 0.6202, 0.5686, 0.5960, 0.6199, 0.6170, 0.6273, 0.6345, 0.6001, 0.6646, 0.6642, 0.5996, 0.6276, 0.5948, 0.6329, 0.6126, 0.6535, 0.6392, 0.6283, 0.6323, 0.6360, 0.5760, 0.5962]
Val_Loss: 0.673989, Val_Acc: 0.605839
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #030, Train_Loss: [0.5563, 0.6319, 0.5902, 0.5958, 0.5715, 0.6408, 0.6205, 0.6525, 0.5167, 0.6164, 0.6427, 0.6682, 0.6260, 0.6291, 0.6287, 0.6101, 0.6372, 0.6192, 0.5827, 0.6426, 0.5902, 0.6387, 0.6480, 0.6036, 0.6272, 0.6250]
Val_Loss: 0.704872, Val_Acc: 0.581509

Epoch #031, Train_Loss: [0.6267, 0.6440, 0.6105, 0.6538, 0.6774, 0.6322, 0.6107, 0.5923, 0.7053, 0.6370, 0.5817, 0.6058, 0.6181, 0.6669, 0.5966, 0.5949, 0.6509, 0.6458, 0.6522, 0.6331, 0.6249, 0.6390, 0.6205, 0.5992, 0.6239, 0.6181]
Val_Loss: 0.668516, Val_Acc: 0.613139
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********

Epoch #032, Train_Loss: [0.6615, 0.6213, 0.6188, 0.6055, 0.6550, 0.6354, 0.6029, 0.5992, 0.6443, 0.5729, 0.5964, 0.6248, 0.5939, 0.6084, 0.6431, 0.5624, 0.5759, 0.6312, 0.5865, 0.6376, 0.6223, 0.6177, 0.6012, 0.6132, 0.6364, 0.6130]
Val_Loss: 0.685174, Val_Acc: 0.605839

Epoch #033, Train_Loss: [0.5800, 0.5685, 0.6059, 0.5970, 0.6543, 0.6112, 0.6502, 0.5722, 0.5955, 0.5503, 0.6449, 0.6077, 0.6025, 0.6159, 0.6344, 0.5872, 0.6444, 0.6059, 0.6539, 0.6107, 0.6092, 0.5769, 0.5919, 0.6238, 0.6092, 0.6326]
Val_Loss: 0.689635, Val_Acc: 0.596107

Epoch #034, Train_Loss: [0.5947, 0.5322, 0.6125, 0.6571, 0.6805, 0.6940, 0.5899, 0.5955, 0.6237, 0.5949, 0.5752, 0.6105, 0.5702, 0.6078, 0.5702, 0.5682, 0.6089, 0.6046, 0.6055, 0.6306, 0.6017, 0.6431, 0.6336, 0.6449, 0.5960, 0.6498]
Val_Loss: 0.683020, Val_Acc: 0.615572

Epoch #035, Train_Loss: [0.5958, 0.6146, 0.6192, 0.5859, 0.5643, 0.6140, 0.5909, 0.5779, 0.5674, 0.6539, 0.6044, 0.5766, 0.6589, 0.6059, 0.5738, 0.6301, 0.6725, 0.6740, 0.6100, 0.6264, 0.6261, 0.6074, 0.6351, 0.5845, 0.6152, 0.6167]
Val_Loss: 0.682065, Val_Acc: 0.610706

Epoch #036, Train_Loss: [0.6416, 0.5861, 0.6389, 0.6500, 0.6503, 0.6207, 0.5964, 0.5836, 0.6041, 0.6278, 0.6382, 0.5761, 0.6279, 0.6512, 0.5843, 0.6333, 0.6067, 0.6008, 0.5961, 0.5853, 0.5827, 0.6186, 0.5392, 0.5861, 0.6265, 0.6696]
Val_Loss: 0.685137, Val_Acc: 0.618005

Epoch #037, Train_Loss: [0.6551, 0.6213, 0.5732, 0.5866, 0.5858, 0.6412, 0.5545, 0.5848, 0.5563, 0.5566, 0.5688, 0.6018, 0.6434, 0.6629, 0.5856, 0.6599, 0.5890, 0.6262, 0.5603, 0.6150, 0.5880, 0.6250, 0.5905, 0.6007, 0.6583, 0.6094]
Val_Loss: 0.692663, Val_Acc: 0.603406

Epoch #038, Train_Loss: [0.5634, 0.6469, 0.6388, 0.6544, 0.6237, 0.6094, 0.5677, 0.5858, 0.6270, 0.6140, 0.6376, 0.6136, 0.5915, 0.6018, 0.6356, 0.6067, 0.5948, 0.5721, 0.6246, 0.6382, 0.6048, 0.6030, 0.6169, 0.6113, 0.6868, 0.5734]
Val_Loss: 0.682485, Val_Acc: 0.632603

Epoch #039, Train_Loss: [0.6442, 0.6576, 0.6116, 0.5931, 0.5534, 0.5850, 0.6172, 0.5940, 0.5967, 0.6510, 0.6174, 0.6069, 0.6106, 0.6159, 0.6184, 0.5713, 0.6154, 0.5785, 0.5798, 0.5829, 0.5882, 0.5375, 0.5953, 0.6617, 0.6497, 0.6236]
Val_Loss: 0.686059, Val_Acc: 0.625304

Epoch #040, Train_Loss: [0.6518, 0.5775, 0.5889, 0.5712, 0.6049, 0.7126, 0.6083, 0.5660, 0.5909, 0.6342, 0.6453, 0.6563, 0.6058, 0.6458, 0.5858, 0.6084, 0.6329, 0.6028, 0.6612, 0.6015, 0.6126, 0.6381, 0.5914, 0.6060, 0.5862, 0.6147]
Val_Loss: 0.676171, Val_Acc: 0.608273

Epoch #041, Train_Loss: [0.6501, 0.6344, 0.6149, 0.5806, 0.5901, 0.6118, 0.6068, 0.5763, 0.5792, 0.6025, 0.5628, 0.5782, 0.6352, 0.6268, 0.5958, 0.5839, 0.6241, 0.5772, 0.6191, 0.6116, 0.6078, 0.6264, 0.5794, 0.6373, 0.6392, 0.6448]
Val_Loss: 0.690950, Val_Acc: 0.622871

Epoch #042, Train_Loss: [0.6167, 0.6294, 0.6025, 0.6125, 0.6190, 0.6292, 0.5430, 0.6288, 0.6310, 0.5789, 0.5741, 0.5461, 0.6458, 0.6322, 0.5992, 0.6259, 0.6459, 0.5713, 0.5880, 0.6174, 0.6458, 0.6061, 0.6366, 0.5530, 0.5741, 0.6943]
Val_Loss: 0.684710, Val_Acc: 0.635036

Epoch #043, Train_Loss: [0.5718, 0.6165, 0.6705, 0.5602, 0.6012, 0.6236, 0.5813, 0.6306, 0.6062, 0.5997, 0.6130, 0.6271, 0.6936, 0.6601, 0.5900, 0.5991, 0.6181, 0.5754, 0.6529, 0.5805, 0.6370, 0.6175, 0.5799, 0.5370, 0.5689, 0.6219]
Val_Loss: 0.682283, Val_Acc: 0.627737

Epoch #044, Train_Loss: [0.5971, 0.5957, 0.6019, 0.5606, 0.6166, 0.6073, 0.6171, 0.6267, 0.6347, 0.6361, 0.6541, 0.6524, 0.6280, 0.6090, 0.6154, 0.6399, 0.5983, 0.5626, 0.6264, 0.5551, 0.6029, 0.5691, 0.6326, 0.6040, 0.6290, 0.5930]
Val_Loss: 0.686054, Val_Acc: 0.635036

Epoch #045, Train_Loss: [0.5895, 0.6234, 0.6266, 0.5959, 0.6255, 0.6173, 0.5934, 0.5978, 0.5733, 0.6141, 0.5887, 0.6255, 0.6061, 0.6203, 0.5823, 0.6338, 0.5856, 0.5842, 0.6245, 0.5969, 0.6250, 0.5948, 0.6375, 0.6007, 0.6056, 0.5847]
Val_Loss: 0.685646, Val_Acc: 0.635036

Epoch #046, Train_Loss: [0.6373, 0.5848, 0.5953, 0.6177, 0.5746, 0.6005, 0.6145, 0.6453, 0.5688, 0.5980, 0.6464, 0.6124, 0.5948, 0.6096, 0.6193, 0.6370, 0.6001, 0.6015, 0.5942, 0.6064, 0.6470, 0.6372, 0.5824, 0.5689, 0.6410, 0.6235]
Val_Loss: 0.698257, Val_Acc: 0.620438

Epoch #047, Train_Loss: [0.5924, 0.6541, 0.5951, 0.6037, 0.6064, 0.5883, 0.6365, 0.5739, 0.6202, 0.5731, 0.6383, 0.6155, 0.6884, 0.6112, 0.6084, 0.5404, 0.6006, 0.6020, 0.6160, 0.7080, 0.6099, 0.6339, 0.5935, 0.5639, 0.6732, 0.5685]
Val_Loss: 0.696048, Val_Acc: 0.627737

Epoch #048, Train_Loss: [0.5674, 0.6132, 0.5766, 0.6270, 0.6454, 0.5963, 0.6770, 0.6707, 0.5913, 0.5425, 0.5724, 0.6593, 0.6172, 0.6332, 0.6365, 0.6416, 0.6130, 0.6265, 0.5977, 0.5822, 0.6148, 0.6549, 0.6531, 0.6254, 0.5928, 0.5868]
Val_Loss: 0.683953, Val_Acc: 0.620438

Epoch #049, Train_Loss: [0.6275, 0.5987, 0.6195, 0.5673, 0.6357, 0.6056, 0.6082, 0.6656, 0.5975, 0.6126, 0.5668, 0.6640, 0.5663, 0.6680, 0.6158, 0.6027, 0.6084, 0.5937, 0.5782, 0.6419, 0.6114, 0.6115, 0.5644, 0.6075, 0.5847, 0.6130]
Val_Loss: 0.704497, Val_Acc: 0.620438

Epoch #050, Train_Loss: [0.5621, 0.6051, 0.5873, 0.6078, 0.6180, 0.5440, 0.6058, 0.6552, 0.6258, 0.6095, 0.6490, 0.6212, 0.6471, 0.5741, 0.6144, 0.6276, 0.5976, 0.6196, 0.6274, 0.5964, 0.6220, 0.6196, 0.5101, 0.6647, 0.6069, 0.6289]
Val_Loss: 0.693035, Val_Acc: 0.618005

Epoch #051, Train_Loss: [0.5835, 0.6350, 0.5540, 0.5957, 0.6285, 0.5852, 0.6206, 0.5767, 0.6403, 0.6475, 0.5924, 0.6184, 0.5799, 0.6001, 0.6301, 0.6066, 0.5465, 0.6335, 0.6532, 0.6500, 0.5625, 0.5937, 0.5478, 0.6680, 0.6272, 0.6038]
Val_Loss: 0.700202, Val_Acc: 0.625304

Epoch #052, Train_Loss: [0.6742, 0.5847, 0.6423, 0.5870, 0.5768, 0.5771, 0.5678, 0.5442, 0.5860, 0.5856, 0.6033, 0.6189, 0.6102, 0.6162, 0.5981, 0.6009, 0.6075, 0.6100, 0.6205, 0.6042, 0.6236, 0.5879, 0.6331, 0.6461, 0.6210, 0.6068]
Val_Loss: 0.699339, Val_Acc: 0.618005

Epoch #053, Train_Loss: [0.6239, 0.6326, 0.6520, 0.5980, 0.5844, 0.5831, 0.6147, 0.6284, 0.5239, 0.6483, 0.6161, 0.6127, 0.6149, 0.6181, 0.5618, 0.5742, 0.5847, 0.5846, 0.6095, 0.5734, 0.5696, 0.6114, 0.6256, 0.5500, 0.6405, 0.5880]
Val_Loss: 0.714223, Val_Acc: 0.620438

Epoch #054, Train_Loss: [0.5566, 0.6369, 0.6177, 0.5443, 0.5799, 0.5801, 0.6406, 0.6121, 0.5422, 0.6043, 0.6286, 0.6777, 0.5879, 0.5775, 0.5912, 0.6253, 0.6422, 0.6230, 0.5742, 0.6047, 0.5832, 0.5590, 0.5824, 0.6482, 0.6233, 0.6400]
Val_Loss: 0.707345, Val_Acc: 0.627737

Epoch #055, Train_Loss: [0.6450, 0.6098, 0.5674, 0.5853, 0.6798, 0.5331, 0.6075, 0.6533, 0.6170, 0.5464, 0.5417, 0.6083, 0.5838, 0.6379, 0.6289, 0.6739, 0.6022, 0.6114, 0.6137, 0.6551, 0.5747, 0.5837, 0.5557, 0.5528, 0.5678, 0.6510]
Val_Loss: 0.710014, Val_Acc: 0.620438

Epoch #056, Train_Loss: [0.5581, 0.6284, 0.5526, 0.5598, 0.6081, 0.5850, 0.6382, 0.5657, 0.6512, 0.5667, 0.5974, 0.5619, 0.5940, 0.5880, 0.6669, 0.5464, 0.6354, 0.5893, 0.5501, 0.6850, 0.6497, 0.5385, 0.6349, 0.6301, 0.6627, 0.5702]
Val_Loss: 0.711655, Val_Acc: 0.620438

Epoch #057, Train_Loss: [0.5934, 0.6048, 0.5995, 0.5884, 0.5662, 0.6291, 0.6498, 0.6479, 0.6243, 0.5926, 0.5981, 0.6397, 0.6129, 0.6168, 0.5626, 0.5769, 0.6185, 0.6019, 0.6153, 0.6032, 0.5481, 0.6146, 0.5857, 0.5960, 0.6380, 0.5785]
Val_Loss: 0.717113, Val_Acc: 0.618005

Epoch #058, Train_Loss: [0.5694, 0.6205, 0.6316, 0.5626, 0.6124, 0.5270, 0.5964, 0.6227, 0.5701, 0.5834, 0.6069, 0.5880, 0.6351, 0.5782, 0.6377, 0.5568, 0.6049, 0.5582, 0.6666, 0.5549, 0.6340, 0.6233, 0.5968, 0.6084, 0.6366, 0.5852]
Val_Loss: 0.717523, Val_Acc: 0.603406

Epoch #059, Train_Loss: [0.5747, 0.5666, 0.6272, 0.6199, 0.6141, 0.6026, 0.5619, 0.5893, 0.6285, 0.5864, 0.5919, 0.6058, 0.6044, 0.6694, 0.6366, 0.6243, 0.5958, 0.5640, 0.5875, 0.5834, 0.5891, 0.5955, 0.5974, 0.5776, 0.6234, 0.6090]
Val_Loss: 0.716881, Val_Acc: 0.622871

Epoch #060, Train_Loss: [0.5851, 0.5999, 0.6033, 0.5964, 0.5998, 0.5665, 0.5892, 0.5767, 0.5688, 0.6115, 0.6045, 0.7010, 0.5415, 0.6567, 0.6263, 0.6144, 0.6095, 0.5894, 0.5850, 0.5771, 0.6310, 0.6096, 0.5636, 0.6209, 0.5825, 0.5862]
Val_Loss: 0.711780, Val_Acc: 0.632603

Epoch #061, Train_Loss: [0.5464, 0.6287, 0.5909, 0.6078, 0.6908, 0.6023, 0.6277, 0.5758, 0.5726, 0.6453, 0.6220, 0.5993, 0.5484, 0.6123, 0.6148, 0.5834, 0.6006, 0.5860, 0.5948, 0.6385, 0.6567, 0.5662, 0.5954, 0.5752, 0.5935, 0.6143]
Val_Loss: 0.718843, Val_Acc: 0.615572

Epoch #062, Train_Loss: [0.5846, 0.5896, 0.6302, 0.5669, 0.6123, 0.6055, 0.6229, 0.5598, 0.5857, 0.6379, 0.5621, 0.6513, 0.6217, 0.5990, 0.5388, 0.6280, 0.6111, 0.5928, 0.5643, 0.6251, 0.5748, 0.6197, 0.6297, 0.5716, 0.5622, 0.7018]
Val_Loss: 0.712058, Val_Acc: 0.630170

Epoch #063, Train_Loss: [0.5701, 0.5891, 0.6221, 0.6278, 0.5400, 0.6340, 0.6105, 0.6200, 0.6566, 0.5913, 0.5911, 0.5998, 0.5712, 0.6648, 0.5689, 0.6072, 0.6257, 0.5936, 0.6099, 0.6388, 0.5939, 0.6323, 0.5976, 0.5847, 0.6484, 0.5450]
Val_Loss: 0.740481, Val_Acc: 0.588808

Epoch #064, Train_Loss: [0.6487, 0.6327, 0.5786, 0.5884, 0.5815, 0.5658, 0.6255, 0.5884, 0.6652, 0.6087, 0.6312, 0.5841, 0.6349, 0.6478, 0.5363, 0.5783, 0.6363, 0.6433, 0.5670, 0.5638, 0.6346, 0.6451, 0.6204, 0.6723, 0.6172, 0.5550]
Val_Loss: 0.709228, Val_Acc: 0.630170

Epoch #065, Train_Loss: [0.5975, 0.5708, 0.5727, 0.5560, 0.6296, 0.6063, 0.5766, 0.5837, 0.5904, 0.5582, 0.6305, 0.6223, 0.6276, 0.5953, 0.5622, 0.5980, 0.5587, 0.6324, 0.5738, 0.6255, 0.6081, 0.6015, 0.6114, 0.6014, 0.6428, 0.6440]
Val_Loss: 0.710404, Val_Acc: 0.635036

Epoch #066, Train_Loss: [0.5677, 0.6600, 0.5779, 0.6072, 0.6107, 0.5495, 0.6396, 0.6037, 0.5806, 0.6057, 0.6217, 0.5946, 0.6279, 0.5261, 0.5897, 0.5824, 0.6445, 0.6172, 0.6477, 0.6122, 0.6223, 0.6070, 0.6420, 0.6184, 0.5672, 0.5573]
Val_Loss: 0.725923, Val_Acc: 0.637470

Epoch #067, Train_Loss: [0.5666, 0.6103, 0.6155, 0.6528, 0.5573, 0.6146, 0.6027, 0.5800, 0.6082, 0.5812, 0.6114, 0.5756, 0.6061, 0.5668, 0.6605, 0.6119, 0.5865, 0.5865, 0.5922, 0.6225, 0.5757, 0.6246, 0.5503, 0.6302, 0.5586, 0.5793]
Val_Loss: 0.718415, Val_Acc: 0.610706

Epoch #068, Train_Loss: [0.5276, 0.5371, 0.5919, 0.5810, 0.6063, 0.5554, 0.5632, 0.5646, 0.5974, 0.6372, 0.6180, 0.5965, 0.6024, 0.6031, 0.6274, 0.6271, 0.5409, 0.6321, 0.6415, 0.6788, 0.5909, 0.5481, 0.6447, 0.6055, 0.5652, 0.6353]
Val_Loss: 0.703801, Val_Acc: 0.622871

Epoch #069, Train_Loss: [0.5732, 0.6267, 0.5725, 0.5694, 0.6341, 0.5773, 0.6306, 0.5875, 0.5727, 0.6020, 0.6183, 0.5806, 0.5836, 0.5751, 0.6036, 0.5895, 0.5934, 0.6291, 0.5845, 0.5745, 0.5709, 0.6312, 0.5371, 0.6073, 0.6258, 0.6359]
Val_Loss: 0.711701, Val_Acc: 0.620438

Epoch #070, Train_Loss: [0.5664, 0.5321, 0.5434, 0.6309, 0.5911, 0.6117, 0.6182, 0.5891, 0.5776, 0.6051, 0.6292, 0.6024, 0.6075, 0.6625, 0.5685, 0.5906, 0.6233, 0.5797, 0.5966, 0.5970, 0.6512, 0.5842, 0.5646, 0.5621, 0.5664, 0.6029]
Val_Loss: 0.712681, Val_Acc: 0.642336

Epoch #071, Train_Loss: [0.6381, 0.6574, 0.6034, 0.6728, 0.6364, 0.5720, 0.6152, 0.5823, 0.5618, 0.5795, 0.6001, 0.5917, 0.6058, 0.6206, 0.5588, 0.5647, 0.6011, 0.5396, 0.6148, 0.5675, 0.5482, 0.6106, 0.6253, 0.6068, 0.6133, 0.6455]
Val_Loss: 0.720542, Val_Acc: 0.642336

Epoch #072, Train_Loss: [0.6861, 0.5869, 0.6050, 0.5783, 0.5398, 0.6019, 0.6111, 0.6027, 0.6120, 0.5924, 0.5489, 0.5581, 0.5271, 0.5808, 0.6421, 0.6176, 0.6454, 0.5727, 0.6518, 0.6043, 0.6098, 0.6039, 0.5626, 0.6051, 0.5138, 0.5442]
Val_Loss: 0.708024, Val_Acc: 0.620438

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/NCI1_Hie/model.pth *********
TEST :: Test_Acc: 0.661800
