+-----------------+-------------------+
|    Parameter    |       Value       |
+=================+===================+
| Batch size      | 128               |
+-----------------+-------------------+
| Dataset         | DD                |
+-----------------+-------------------+
| Dropout ratio   | 0.5               |
+-----------------+-------------------+
| Epochs          | 10000             |
+-----------------+-------------------+
| Exp name        | DD_Glo            |
+-----------------+-------------------+
| Gpu index       | 0                 |
+-----------------+-------------------+
| Hid             | 128               |
+-----------------+-------------------+
| Lr              | 0.0005            |
+-----------------+-------------------+
| Model           | SAGPooling_Global |
+-----------------+-------------------+
| Patience        | 40                |
+-----------------+-------------------+
| Pooling ratio   | 0.5               |
+-----------------+-------------------+
| Seed            | 16                |
+-----------------+-------------------+
| Test batch size | 1                 |
+-----------------+-------------------+
| Weight decay    | 0.0001            |
+-----------------+-------------------+
Using GPU: 0
SAGPooling_Global(
  (conv1): GCNConv(89, 128)
  (conv2): GCNConv(128, 128)
  (conv3): GCNConv(128, 128)
  (pool): SAGPooling(GCNConv, 384, ratio=0.5, multiplier=1.0)
  (lin1): Linear(in_features=768, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Model Parameter: 151748
Using Adam

Epoch #000, Train_Loss: [0.6904, 0.6817, 0.6793, 0.6887, 0.6896, 0.6799, 0.6908, 0.6624]
Val_Loss: 0.691181, Val_Acc: 0.529915
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #001, Train_Loss: [0.6852, 0.6849, 0.6843, 0.6834, 0.6743, 0.6750, 0.6795, 0.6598]
Val_Loss: 0.691576, Val_Acc: 0.529915

Epoch #002, Train_Loss: [0.6778, 0.6827, 0.6748, 0.6718, 0.6903, 0.6628, 0.6666, 0.6647]
Val_Loss: 0.696339, Val_Acc: 0.529915

Epoch #003, Train_Loss: [0.6764, 0.6584, 0.6733, 0.6672, 0.6894, 0.6629, 0.6616, 0.6575]
Val_Loss: 0.706827, Val_Acc: 0.529915

Epoch #004, Train_Loss: [0.6988, 0.6294, 0.6740, 0.6644, 0.6770, 0.6658, 0.6611, 0.6891]
Val_Loss: 0.703531, Val_Acc: 0.529915

Epoch #005, Train_Loss: [0.6837, 0.6534, 0.6522, 0.6966, 0.6529, 0.6516, 0.6687, 0.6830]
Val_Loss: 0.698029, Val_Acc: 0.529915

Epoch #006, Train_Loss: [0.6947, 0.6089, 0.6385, 0.6801, 0.6564, 0.6554, 0.7005, 0.7015]
Val_Loss: 0.700083, Val_Acc: 0.529915

Epoch #007, Train_Loss: [0.6796, 0.6603, 0.6780, 0.6433, 0.6478, 0.6551, 0.6510, 0.6804]
Val_Loss: 0.697773, Val_Acc: 0.529915

Epoch #008, Train_Loss: [0.6422, 0.6550, 0.6757, 0.6735, 0.6214, 0.6550, 0.6638, 0.6545]
Val_Loss: 0.700803, Val_Acc: 0.529915

Epoch #009, Train_Loss: [0.6717, 0.6458, 0.6894, 0.6309, 0.6322, 0.6080, 0.6943, 0.5734]
Val_Loss: 0.702341, Val_Acc: 0.529915

Epoch #010, Train_Loss: [0.6510, 0.6491, 0.6264, 0.6381, 0.6471, 0.6535, 0.6125, 0.6809]
Val_Loss: 0.703042, Val_Acc: 0.529915

Epoch #011, Train_Loss: [0.6320, 0.6394, 0.6481, 0.6155, 0.6295, 0.6487, 0.6112, 0.5937]
Val_Loss: 0.727687, Val_Acc: 0.529915

Epoch #012, Train_Loss: [0.6481, 0.5759, 0.5790, 0.6320, 0.7075, 0.6525, 0.6014, 0.6070]
Val_Loss: 0.706673, Val_Acc: 0.529915

Epoch #013, Train_Loss: [0.6018, 0.5758, 0.6449, 0.6040, 0.6775, 0.6060, 0.6202, 0.5926]
Val_Loss: 0.730505, Val_Acc: 0.581197

Epoch #014, Train_Loss: [0.5803, 0.6122, 0.6144, 0.6551, 0.6382, 0.5761, 0.5906, 0.7131]
Val_Loss: 0.699429, Val_Acc: 0.623932

Epoch #015, Train_Loss: [0.5882, 0.6207, 0.6089, 0.5941, 0.6129, 0.5643, 0.6258, 0.5447]
Val_Loss: 0.716849, Val_Acc: 0.623932

Epoch #016, Train_Loss: [0.6338, 0.5679, 0.6144, 0.6054, 0.5682, 0.5827, 0.5844, 0.5464]
Val_Loss: 0.727626, Val_Acc: 0.649573

Epoch #017, Train_Loss: [0.5968, 0.5498, 0.5730, 0.5960, 0.5958, 0.5615, 0.5738, 0.5267]
Val_Loss: 0.709084, Val_Acc: 0.649573

Epoch #018, Train_Loss: [0.5571, 0.5712, 0.5057, 0.5708, 0.5881, 0.5755, 0.4990, 0.6059]
Val_Loss: 0.679804, Val_Acc: 0.632479
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #019, Train_Loss: [0.5436, 0.5108, 0.6109, 0.4726, 0.5572, 0.5547, 0.6012, 0.5905]
Val_Loss: 0.698829, Val_Acc: 0.589744

Epoch #020, Train_Loss: [0.5471, 0.5710, 0.5944, 0.5499, 0.5568, 0.5147, 0.5856, 0.6208]
Val_Loss: 0.683545, Val_Acc: 0.658120

Epoch #021, Train_Loss: [0.5642, 0.5218, 0.5634, 0.5672, 0.5833, 0.5093, 0.5105, 0.5015]
Val_Loss: 0.640888, Val_Acc: 0.675214
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #022, Train_Loss: [0.5532, 0.5298, 0.5459, 0.5147, 0.4750, 0.5587, 0.5333, 0.4863]
Val_Loss: 0.649403, Val_Acc: 0.675214

Epoch #023, Train_Loss: [0.5541, 0.5141, 0.4602, 0.5613, 0.5672, 0.5574, 0.5029, 0.5191]
Val_Loss: 0.650965, Val_Acc: 0.649573

Epoch #024, Train_Loss: [0.5362, 0.5327, 0.6026, 0.5509, 0.5069, 0.5172, 0.5644, 0.5268]
Val_Loss: 0.685041, Val_Acc: 0.641026

Epoch #025, Train_Loss: [0.5033, 0.5352, 0.4687, 0.5416, 0.5423, 0.5202, 0.5166, 0.5575]
Val_Loss: 0.615718, Val_Acc: 0.683761
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #026, Train_Loss: [0.5573, 0.5077, 0.5035, 0.4945, 0.5548, 0.5022, 0.5292, 0.5238]
Val_Loss: 0.619830, Val_Acc: 0.675214

Epoch #027, Train_Loss: [0.5108, 0.4742, 0.4971, 0.4822, 0.6048, 0.5312, 0.5776, 0.3828]
Val_Loss: 0.674256, Val_Acc: 0.675214

Epoch #028, Train_Loss: [0.4815, 0.5957, 0.4886, 0.4713, 0.5658, 0.5156, 0.5048, 0.3779]
Val_Loss: 0.614335, Val_Acc: 0.692308
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #029, Train_Loss: [0.5576, 0.5690, 0.4311, 0.4709, 0.4576, 0.5702, 0.4300, 0.5706]
Val_Loss: 0.640794, Val_Acc: 0.666667

Epoch #030, Train_Loss: [0.4759, 0.4569, 0.5164, 0.4946, 0.5056, 0.5354, 0.4770, 0.5548]
Val_Loss: 0.592531, Val_Acc: 0.735043
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #031, Train_Loss: [0.4955, 0.5371, 0.5313, 0.5084, 0.5048, 0.4581, 0.4373, 0.3598]
Val_Loss: 0.587456, Val_Acc: 0.752137
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #032, Train_Loss: [0.4953, 0.5306, 0.4705, 0.4539, 0.4964, 0.4534, 0.5293, 0.4271]
Val_Loss: 0.605856, Val_Acc: 0.743590

Epoch #033, Train_Loss: [0.4291, 0.5176, 0.5654, 0.5413, 0.4280, 0.4474, 0.5576, 0.5213]
Val_Loss: 0.644201, Val_Acc: 0.700855

Epoch #034, Train_Loss: [0.5199, 0.4404, 0.4748, 0.5502, 0.5002, 0.4778, 0.4847, 0.4343]
Val_Loss: 0.577937, Val_Acc: 0.726496
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #035, Train_Loss: [0.4916, 0.4910, 0.4458, 0.4287, 0.4680, 0.5313, 0.5454, 0.4015]
Val_Loss: 0.567293, Val_Acc: 0.743590
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #036, Train_Loss: [0.4831, 0.4235, 0.4908, 0.5537, 0.4375, 0.4856, 0.4652, 0.4586]
Val_Loss: 0.562957, Val_Acc: 0.735043
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #037, Train_Loss: [0.5470, 0.5030, 0.4930, 0.4598, 0.5096, 0.4723, 0.3915, 0.3529]
Val_Loss: 0.551178, Val_Acc: 0.735043
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #038, Train_Loss: [0.4866, 0.4287, 0.5168, 0.4990, 0.4355, 0.4871, 0.4520, 0.3643]
Val_Loss: 0.605055, Val_Acc: 0.717949

Epoch #039, Train_Loss: [0.4293, 0.4947, 0.4290, 0.5078, 0.4596, 0.5211, 0.4852, 0.4236]
Val_Loss: 0.595324, Val_Acc: 0.726496

Epoch #040, Train_Loss: [0.5172, 0.4902, 0.3933, 0.5437, 0.4853, 0.4622, 0.4597, 0.5905]
Val_Loss: 0.533724, Val_Acc: 0.769231
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #041, Train_Loss: [0.4460, 0.4572, 0.4488, 0.4635, 0.5045, 0.4307, 0.4839, 0.4579]
Val_Loss: 0.537106, Val_Acc: 0.752137

Epoch #042, Train_Loss: [0.4110, 0.5465, 0.4771, 0.4204, 0.4814, 0.4114, 0.4974, 0.3749]
Val_Loss: 0.561039, Val_Acc: 0.752137

Epoch #043, Train_Loss: [0.3964, 0.4844, 0.4491, 0.4732, 0.4599, 0.4510, 0.4345, 0.3879]
Val_Loss: 0.557922, Val_Acc: 0.752137

Epoch #044, Train_Loss: [0.3856, 0.3689, 0.4722, 0.4230, 0.4694, 0.5269, 0.4908, 0.4309]
Val_Loss: 0.534430, Val_Acc: 0.752137

Epoch #045, Train_Loss: [0.4365, 0.3815, 0.4824, 0.4554, 0.4108, 0.5047, 0.4409, 0.3623]
Val_Loss: 0.533687, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #046, Train_Loss: [0.4405, 0.4837, 0.4510, 0.4334, 0.4676, 0.3665, 0.5066, 0.5007]
Val_Loss: 0.531785, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #047, Train_Loss: [0.4615, 0.4050, 0.4392, 0.4495, 0.5110, 0.4494, 0.4312, 0.4026]
Val_Loss: 0.535809, Val_Acc: 0.769231

Epoch #048, Train_Loss: [0.4244, 0.4591, 0.5014, 0.4698, 0.4366, 0.4109, 0.3744, 0.3640]
Val_Loss: 0.522442, Val_Acc: 0.752137
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #049, Train_Loss: [0.4399, 0.3935, 0.3847, 0.5255, 0.4301, 0.4006, 0.4538, 0.4644]
Val_Loss: 0.533500, Val_Acc: 0.769231

Epoch #050, Train_Loss: [0.3888, 0.4550, 0.4174, 0.4417, 0.4596, 0.4534, 0.3641, 0.4639]
Val_Loss: 0.520975, Val_Acc: 0.760684
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #051, Train_Loss: [0.5033, 0.3791, 0.4815, 0.4016, 0.3641, 0.4080, 0.4875, 0.3972]
Val_Loss: 0.514112, Val_Acc: 0.769231
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #052, Train_Loss: [0.4485, 0.4355, 0.5632, 0.4658, 0.4127, 0.4059, 0.4893, 0.3779]
Val_Loss: 0.522113, Val_Acc: 0.769231

Epoch #053, Train_Loss: [0.3633, 0.4218, 0.4151, 0.4221, 0.5134, 0.4513, 0.4338, 0.2936]
Val_Loss: 0.540522, Val_Acc: 0.760684

Epoch #054, Train_Loss: [0.4125, 0.3899, 0.4603, 0.4028, 0.4874, 0.4025, 0.3859, 0.3572]
Val_Loss: 0.528683, Val_Acc: 0.777778

Epoch #055, Train_Loss: [0.4012, 0.3843, 0.4651, 0.4332, 0.3814, 0.4233, 0.4481, 0.4185]
Val_Loss: 0.513350, Val_Acc: 0.769231
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #056, Train_Loss: [0.4774, 0.4746, 0.5136, 0.4194, 0.3645, 0.3188, 0.3578, 0.4563]
Val_Loss: 0.525393, Val_Acc: 0.769231

Epoch #057, Train_Loss: [0.3696, 0.3661, 0.4072, 0.3762, 0.4645, 0.4683, 0.4256, 0.3151]
Val_Loss: 0.537711, Val_Acc: 0.769231

Epoch #058, Train_Loss: [0.3976, 0.4494, 0.3446, 0.3972, 0.4070, 0.4287, 0.4142, 0.4179]
Val_Loss: 0.512749, Val_Acc: 0.769231
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #059, Train_Loss: [0.3151, 0.4310, 0.5010, 0.3960, 0.4700, 0.3939, 0.3336, 0.3007]
Val_Loss: 0.504723, Val_Acc: 0.777778
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #060, Train_Loss: [0.4321, 0.3504, 0.3492, 0.4837, 0.4638, 0.3931, 0.3916, 0.2969]
Val_Loss: 0.503248, Val_Acc: 0.769231
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #061, Train_Loss: [0.3810, 0.3639, 0.3828, 0.4443, 0.4105, 0.5219, 0.3745, 0.5099]
Val_Loss: 0.503772, Val_Acc: 0.786325

Epoch #062, Train_Loss: [0.3941, 0.3919, 0.4539, 0.3083, 0.3948, 0.4431, 0.4261, 0.3177]
Val_Loss: 0.505677, Val_Acc: 0.769231

Epoch #063, Train_Loss: [0.4144, 0.4623, 0.3858, 0.3496, 0.3819, 0.4268, 0.4195, 0.3004]
Val_Loss: 0.508816, Val_Acc: 0.786325

Epoch #064, Train_Loss: [0.3304, 0.3877, 0.3969, 0.3709, 0.3390, 0.4743, 0.4642, 0.3378]
Val_Loss: 0.498994, Val_Acc: 0.777778
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #065, Train_Loss: [0.4035, 0.4166, 0.3943, 0.3767, 0.4059, 0.3859, 0.3609, 0.2923]
Val_Loss: 0.508645, Val_Acc: 0.786325

Epoch #066, Train_Loss: [0.4094, 0.3299, 0.3957, 0.4869, 0.3393, 0.4013, 0.2916, 0.4619]
Val_Loss: 0.496700, Val_Acc: 0.777778
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #067, Train_Loss: [0.3300, 0.3565, 0.3748, 0.4053, 0.3985, 0.3985, 0.3566, 0.5039]
Val_Loss: 0.496542, Val_Acc: 0.794872
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #068, Train_Loss: [0.3755, 0.3656, 0.3192, 0.3978, 0.3837, 0.3792, 0.3608, 0.4761]
Val_Loss: 0.508925, Val_Acc: 0.777778

Epoch #069, Train_Loss: [0.3546, 0.3762, 0.2873, 0.4593, 0.4215, 0.3148, 0.3763, 0.4219]
Val_Loss: 0.498395, Val_Acc: 0.769231

Epoch #070, Train_Loss: [0.3791, 0.4371, 0.3425, 0.4058, 0.3355, 0.3274, 0.3564, 0.2605]
Val_Loss: 0.517606, Val_Acc: 0.777778

Epoch #071, Train_Loss: [0.3512, 0.3254, 0.3319, 0.4721, 0.3964, 0.3604, 0.4238, 0.4104]
Val_Loss: 0.532025, Val_Acc: 0.769231

Epoch #072, Train_Loss: [0.3715, 0.3641, 0.3234, 0.4082, 0.4123, 0.3850, 0.3469, 0.2991]
Val_Loss: 0.505324, Val_Acc: 0.786325

Epoch #073, Train_Loss: [0.3730, 0.4623, 0.2820, 0.3797, 0.3507, 0.3515, 0.3477, 0.4186]
Val_Loss: 0.530867, Val_Acc: 0.786325

Epoch #074, Train_Loss: [0.3876, 0.4724, 0.3525, 0.3467, 0.3825, 0.3309, 0.2926, 0.3473]
Val_Loss: 0.509921, Val_Acc: 0.777778

Epoch #075, Train_Loss: [0.2849, 0.3446, 0.3114, 0.3551, 0.3953, 0.4144, 0.3648, 0.4675]
Val_Loss: 0.502939, Val_Acc: 0.786325

Epoch #076, Train_Loss: [0.4260, 0.3656, 0.2776, 0.3380, 0.3809, 0.2737, 0.3393, 0.4514]
Val_Loss: 0.500751, Val_Acc: 0.786325

Epoch #077, Train_Loss: [0.4132, 0.3110, 0.2605, 0.3168, 0.3979, 0.2872, 0.4644, 0.4332]
Val_Loss: 0.519276, Val_Acc: 0.786325

Epoch #078, Train_Loss: [0.3553, 0.4100, 0.3567, 0.3083, 0.2680, 0.4043, 0.3589, 0.3574]
Val_Loss: 0.485470, Val_Acc: 0.811966
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********

Epoch #079, Train_Loss: [0.4068, 0.3778, 0.3503, 0.3629, 0.3467, 0.3445, 0.3079, 0.3857]
Val_Loss: 0.488287, Val_Acc: 0.803419

Epoch #080, Train_Loss: [0.3077, 0.4015, 0.2668, 0.3621, 0.3900, 0.3993, 0.3228, 0.3445]
Val_Loss: 0.489670, Val_Acc: 0.803419

Epoch #081, Train_Loss: [0.3398, 0.3498, 0.4063, 0.2756, 0.4350, 0.2477, 0.4051, 0.4553]
Val_Loss: 0.491915, Val_Acc: 0.794872

Epoch #082, Train_Loss: [0.4079, 0.3719, 0.3939, 0.3110, 0.3403, 0.3329, 0.3587, 0.3281]
Val_Loss: 0.491734, Val_Acc: 0.786325

Epoch #083, Train_Loss: [0.3589, 0.4004, 0.4018, 0.2824, 0.2959, 0.3335, 0.3204, 0.1866]
Val_Loss: 0.531878, Val_Acc: 0.786325

Epoch #084, Train_Loss: [0.3154, 0.3069, 0.3383, 0.3548, 0.3530, 0.3616, 0.3887, 0.2950]
Val_Loss: 0.549061, Val_Acc: 0.752137

Epoch #085, Train_Loss: [0.3795, 0.4457, 0.3404, 0.3446, 0.2973, 0.3629, 0.2502, 0.2998]
Val_Loss: 0.578855, Val_Acc: 0.752137

Epoch #086, Train_Loss: [0.4049, 0.3278, 0.3906, 0.3480, 0.4090, 0.2852, 0.3100, 0.4378]
Val_Loss: 0.608189, Val_Acc: 0.726496

Epoch #087, Train_Loss: [0.4035, 0.4217, 0.2976, 0.3062, 0.3582, 0.4213, 0.3244, 0.4386]
Val_Loss: 0.531559, Val_Acc: 0.786325

Epoch #088, Train_Loss: [0.3394, 0.3650, 0.4656, 0.3388, 0.3374, 0.2759, 0.3514, 0.3717]
Val_Loss: 0.490724, Val_Acc: 0.794872

Epoch #089, Train_Loss: [0.4035, 0.3550, 0.3231, 0.3619, 0.3989, 0.3455, 0.3082, 0.2570]
Val_Loss: 0.488276, Val_Acc: 0.803419

Epoch #090, Train_Loss: [0.3752, 0.3489, 0.3327, 0.3268, 0.2736, 0.2883, 0.3836, 0.3110]
Val_Loss: 0.509934, Val_Acc: 0.794872

Epoch #091, Train_Loss: [0.3254, 0.2671, 0.3525, 0.3407, 0.2372, 0.3856, 0.2907, 0.4182]
Val_Loss: 0.506326, Val_Acc: 0.803419

Epoch #092, Train_Loss: [0.2928, 0.2918, 0.3000, 0.3664, 0.3553, 0.2621, 0.3239, 0.2686]
Val_Loss: 0.504134, Val_Acc: 0.803419

Epoch #093, Train_Loss: [0.3347, 0.2951, 0.3328, 0.3296, 0.2929, 0.3209, 0.2659, 0.3415]
Val_Loss: 0.519940, Val_Acc: 0.803419

Epoch #094, Train_Loss: [0.2783, 0.3567, 0.2905, 0.2684, 0.3474, 0.2541, 0.3525, 0.2640]
Val_Loss: 0.500450, Val_Acc: 0.786325

Epoch #095, Train_Loss: [0.2746, 0.3530, 0.3148, 0.3693, 0.2879, 0.2771, 0.2970, 0.2280]
Val_Loss: 0.517501, Val_Acc: 0.811966

Epoch #096, Train_Loss: [0.2897, 0.3217, 0.2428, 0.3899, 0.3074, 0.2797, 0.2748, 0.3109]
Val_Loss: 0.570888, Val_Acc: 0.752137

Epoch #097, Train_Loss: [0.3936, 0.3767, 0.3058, 0.2582, 0.2941, 0.2806, 0.2622, 0.3740]
Val_Loss: 0.525164, Val_Acc: 0.811966

Epoch #098, Train_Loss: [0.3308, 0.2928, 0.2807, 0.2841, 0.3931, 0.2440, 0.2287, 0.3968]
Val_Loss: 0.508486, Val_Acc: 0.811966

Epoch #099, Train_Loss: [0.2913, 0.3261, 0.2904, 0.2310, 0.2408, 0.3176, 0.3599, 0.2929]
Val_Loss: 0.509624, Val_Acc: 0.811966

Epoch #100, Train_Loss: [0.4041, 0.2532, 0.2543, 0.3664, 0.2575, 0.3256, 0.2758, 0.2069]
Val_Loss: 0.530259, Val_Acc: 0.811966

Epoch #101, Train_Loss: [0.2497, 0.3254, 0.2922, 0.3139, 0.2439, 0.2701, 0.4248, 0.3400]
Val_Loss: 0.504886, Val_Acc: 0.803419

Epoch #102, Train_Loss: [0.2451, 0.3545, 0.3217, 0.2384, 0.2951, 0.3022, 0.4182, 0.2846]
Val_Loss: 0.505806, Val_Acc: 0.794872

Epoch #103, Train_Loss: [0.3347, 0.2350, 0.3210, 0.3512, 0.2832, 0.2958, 0.2906, 0.2126]
Val_Loss: 0.504942, Val_Acc: 0.794872

Epoch #104, Train_Loss: [0.3421, 0.3128, 0.2490, 0.3341, 0.2968, 0.3713, 0.2313, 0.2998]
Val_Loss: 0.510119, Val_Acc: 0.794872

Epoch #105, Train_Loss: [0.2882, 0.2797, 0.3125, 0.3036, 0.3973, 0.2847, 0.2372, 0.3910]
Val_Loss: 0.505104, Val_Acc: 0.794872

Epoch #106, Train_Loss: [0.2727, 0.2465, 0.2682, 0.3365, 0.3573, 0.3182, 0.3211, 0.3128]
Val_Loss: 0.509442, Val_Acc: 0.794872

Epoch #107, Train_Loss: [0.2197, 0.3217, 0.2882, 0.3433, 0.2336, 0.2844, 0.3846, 0.3990]
Val_Loss: 0.515575, Val_Acc: 0.803419

Epoch #108, Train_Loss: [0.2964, 0.2352, 0.2299, 0.3685, 0.3699, 0.2397, 0.4113, 0.3194]
Val_Loss: 0.521311, Val_Acc: 0.794872

Epoch #109, Train_Loss: [0.3262, 0.2989, 0.2951, 0.3376, 0.3618, 0.2799, 0.2783, 0.2935]
Val_Loss: 0.522182, Val_Acc: 0.777778

Epoch #110, Train_Loss: [0.2667, 0.2857, 0.2819, 0.2105, 0.4829, 0.3013, 0.3138, 0.3843]
Val_Loss: 0.508726, Val_Acc: 0.794872

Epoch #111, Train_Loss: [0.2810, 0.2645, 0.3004, 0.1991, 0.2282, 0.4250, 0.3147, 0.1931]
Val_Loss: 0.510266, Val_Acc: 0.803419

Epoch #112, Train_Loss: [0.2481, 0.3092, 0.2905, 0.2058, 0.3203, 0.2108, 0.3107, 0.3561]
Val_Loss: 0.517880, Val_Acc: 0.794872

Epoch #113, Train_Loss: [0.2944, 0.2562, 0.2574, 0.3539, 0.2419, 0.2516, 0.2238, 0.2422]
Val_Loss: 0.526087, Val_Acc: 0.794872

Epoch #114, Train_Loss: [0.2177, 0.2523, 0.3141, 0.2561, 0.2690, 0.2332, 0.3076, 0.2507]
Val_Loss: 0.534675, Val_Acc: 0.794872

Epoch #115, Train_Loss: [0.2327, 0.3289, 0.2630, 0.3021, 0.2274, 0.2510, 0.2945, 0.2026]
Val_Loss: 0.530127, Val_Acc: 0.811966

Epoch #116, Train_Loss: [0.2563, 0.3646, 0.3683, 0.2394, 0.2399, 0.2721, 0.2455, 0.4027]
Val_Loss: 0.520344, Val_Acc: 0.803419

Epoch #117, Train_Loss: [0.2845, 0.2899, 0.3141, 0.1575, 0.1973, 0.3002, 0.3670, 0.2856]
Val_Loss: 0.521271, Val_Acc: 0.811966

Epoch #118, Train_Loss: [0.2672, 0.2487, 0.2543, 0.2699, 0.2906, 0.2857, 0.2708, 0.2346]
Val_Loss: 0.523601, Val_Acc: 0.811966

Epoch #119, Train_Loss: [0.3051, 0.2491, 0.2232, 0.3191, 0.2272, 0.2147, 0.2756, 0.2997]
Val_Loss: 0.547884, Val_Acc: 0.786325

********** TEST START **********
Reload Best Model
The current best model is saved in: ******** outputs/DD_Glo/model.pth *********
TEST :: Test_Acc: 0.731092
